[
["index.html", "Kvantitative metoder for de finansielle uddannelser.", " Kvantitative metoder for de finansielle uddannelser. Thomas Petersen 2018-10-10 "],
["sandsynligheder.html", "Kapitel 1 Sandsynligheder 1.1 Betingede sandsynligheder 1.2 Uafhængighed", " Kapitel 1 Sandsynligheder .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Vi vil her se på sandsynligheder. Hvis vi forestiller os, man har på en cafe har spurgt 800 besøgende, om de drikker the, kaffe eller begge dele, og har fået følgende svar. Kaffe The Både kaffe og the 600 400 200 Vi kan opstille resultaterne i et Venn-diagram som vist på figuren. Vi kan udføre et stokastisk eksperiment og tilfældigt udtage en person og undersøge, hvor stor sandsynligheden er for at vedkommende drikker kaffe. Vi skriver P for probability dvs. sandsynlighed. Sandsynligheden for en person drikker kaffe kan opskrives som: \\[\\small P(Kaffe)=\\frac{Antal\\ kaffe-drikkere}{Antal\\ adspurgte}=\\frac{600}{800}=0.75=75\\%\\] På samme måde kan man finde sandsynligheden for at en person drikker the: \\[\\small P(The)=\\frac{Antal\\ the-drikkere}{Antal\\ adspurgte}=\\frac{400}{800}=0.5=50\\%\\] 1.0.1 Forenings- og fælleshændelser Sandsynligheden for at en person drikker the eller kaffe, er en foreningshændelse (vi bruger symbolet \\(\\cup\\)), det skriver vi som: \\[P(The\\cup Kaffe)=\\frac{Antal\\ the- eller\\ kaffedrikkere}{Antal\\ adspurgte}=\\frac{800}{800}=1\\] Alle der kommer på denne cafe, drikker altså enten the eller kaffe. Sandsynligheden for at en person drikker the og kaffe, er en fælleshændelse (vi bruger symnolet\\(\\cap\\)), det skriver vi som: \\[P(The\\cap Kaffe)=\\frac{Antal\\ the- og\\ kaffedrikkere}{Antal\\ adspurgte}=\\frac{200}{800}=0.25\\] Har man svært ved at huske betydningen af de 2 symboler. Så tænk på at foreningshændelsen \\(\\cup\\) (det ligner er jo en kop), kan rumme mere end fælleshændelsen \\(\\cap\\) (der er ikke meget plads på toppen). 1.1 Betingede sandsynligheder Vi kan også se på en mindre gruppe af de adspurgte, hvis fx. man kun vil se på kaffedrikkerne kunne man ønske at undersøge, hvor stor en andel af kaffedrikkerne der både drikker the og kaffe. Vi siger at vi betinger med en hændelse, hvis vi ser på en sådan delmængde af de adspurgte. Betingede sandsynligheder kan formuleres på flere måder fx: Givet at man drikker kaffe hvad er da sandsynligheden for at man ligeledes drikker the? Hvis man drikker kaffe hvad er da sandsynligheden for at man ligeledes drikker the? Hvad er da sandsynligheden for, at man drikker the, når man drikker kaffe? Hvad er andelen af kaffedrikkere, der drikker the? Vi siger vi betinger med hændelsen A, man drikker kaffe, og ser således kun på gruppen af kaffedrikkere. Hvad er så sandsynligheden for at man også drikker the hændelsen B? Vi skal altså bestemme sandsynligheden for B givet A, med symboler skriver vi det som \\(\\small P(B\\mid A)\\). Vi kan udregne dette som \\[\\small P(B\\mid A)=\\frac{P(B\\cap A)}{P(A)}\\] Det betyder vi kan udregne hvad er andelen af kaffedrikkere, der drikker the, ved formlen: \\[\\small P(B\\mid A)=P(kaffe\\ og\\ the\\mid kaffe)=\\frac{P(kaffe\\ og\\ the\\cap kaffe)}{P(kaffe)}\\] \\[\\small \\frac{\\frac{200}{800}}{\\frac{600}{800}}=\\frac{200}{600}=\\frac{1}{3}\\approx 33\\%\\] 1.1.1 Race og dødsdom eksempel I et kendt studie fra 1991 af Radelet and Pierce, om Florida, har man indsamlet data i retssager hvor der var mulighed for dødsstraf. Man har blandt andet indsamlet data om den tiltaltes etnicitet, offerets etnicitet samt udfaldet af retssagen dvs. om den tiltalte blev idømt dødstraf eller ikke. Studiets formål var at afgøre om der ved domstolene er en bias, således at amerikanere af afrikansk afstamning oftere idømmmes dødsstraf end hvide. Race tiltalt Dødsdom Ikke dødsdom Total Hvid 53 430 483 Afrikansk amerikaner 15 176 191 Total 68 606 674 Sandsynligheden for at blive dødsdømt, når man er tiltalt kan udregnes til: \\[\\small P(dø\\ dsdom)=\\frac{dø\\ dsdø\\ mte}{alle\\ tiltalte}=\\frac{68}{674}=0.1009\\] Sandsynligheden for at blive dødsdømt, når man er hvid og tiltalt er en betinget sandsynlighed, denne kan udregnes til: \\[\\small P(dø\\ dsdom\\mid hvid)=\\frac{P(dø\\ dsdom \\cap hvid)}{P(hvide\\ tiltalte)}=\\frac{53}{483}=0.1097\\] Sandsynligheden for at blive dødsdømt, hvis man er afrikansk-amerikaner og tiltalt er en betinget sandsynlighed, denne kan udregnes til: \\[\\small P(dø\\ dsdom\\mid sort)=\\frac{P(dø\\ dsdom\\cap sort)}{P(sorte\\ tiltalte)}=\\frac{15}{191}=0.0785\\] Dette tyder ikke på at der er racemæssig forskel på andelen af dødsdømte. 1.2 Uafhængighed Vi kan sige at 2 hændelser A og B er uafhængige hvis sandsynligheden for at den ene hændelse indtræffer ikke påvirkes af om den anden hændelse indtræffer eller ej. Sandsynligheden for at slå krone med en mønt og slå en sekser med en terning, er 2 hændelser der er uafhængige. Trækker man et es fra et spil kort, vil sandsynligheden for igen at trække et es være påvirket af det første træk. Disse hændelser er ikke uafhængige. Vi kan udtrykke uafhængige hændelser vha. betingede sandsynligheder. \\[\\small P(A\\mid B)=P(A)\\] Her står sandsynligheden for A er den samme ligegyldigt om hændelsen B indtræffer eller ej. \\[\\small P(B\\mid A)=P(B)\\] Her står sandsynligheden for B er den samme ligegyldigt om hændelsen A indtræffer eller ej. Vi kan omskrive begge ligninger til nedenstående resultat. \\[\\small P(A\\mid B)=P(A)\\Leftrightarrow \\frac{P(A\\cap B)}{P(B)}=P(A)\\Leftrightarrow P(A\\cap B)=P(A)\\cdot P(B)\\] Så hændelserne A og B er uafhængige hvis fælleshændelsen \\(\\small P(A\\cap B)\\) er lig med produktet af hændelserne \\(\\small P(A) \\cdot P(B)\\) Er hændelserne drikke kaffe og the uafhængige? Vi finder sandsynligheden for fælleshændelsen: \\[\\small P(Kaffe \\cap The)=\\frac{200}{800}=0.25\\] Er sandsynligheden for fælleshændelsen lig med produktet af sandsynlighederne? \\[\\small P(Kaffe)\\cdot P(The)=\\frac{600}{800}\\cdot \\frac{400}{800}=\\frac{3}{4}\\cdot \\frac{1}{2}=\\frac{3}{8}=0.375\\] Hændelserne er altså ikke uafhængige for 0.25 er ikke 0.375. Vi ser på om hændelserne hvid tiltalt og dødsdom er afhængige. Vi finder sandsynligheden for fælleshændelsen: \\[\\small P(Dø\\ dsdom \\cap Hvid)=\\frac{53}{674}=0.08\\] Er sandsynligheden for fælleshændelsen lig med produktet af sandsynlighederne? \\[\\small P(Dø\\ dsdom)\\cdot P(Hvid)=\\frac{68}{674}\\cdot \\frac{483}{674}=0.10 \\cdot 0.72=0.072\\] Sandsynlighederne 0.08 og 0.072 er tæt på hinanden, udtaler vi os om en population ville vi nok ikke kunne afvise at hændelserne er uafhængige. Vi vil i afsnittede om Chi i anden tests se på hvorledes man tester uafhængighed. Spørgsmål dødsdom Vi inddeler nu skemaet ovenfor, med en variabel, der angiver offerets race. Race offer Race tiltalt Dødsdom Ikke dødsdom Total Hvid Hvid 53 414 467 Hvid Afrikansk amerikaner 11 37 48 Afrikansk amerikaner Hvid 0 16 16 Afrikansk amerikaner Afrikansk amerikaner 4 139 143 Total 68 606 674 1. Hvad er sandsynligheden for at blive dødsdømt, hvis man er hvid tiltalt for at have dræbt en hvid? 2. Hvad er sandsynligheden for at blive dødsdømt, givet man er sort tiltalt for at have dræbt en hvid? 3. Hvad er sandsynligheden for at blive dødsdømt, givet man er hvid tiltalt for at have dræbt en sort? 4. Hvad er sandsynligheden for at blive dødsdømt, når man er sort tiltalt for at have dræbt en sort? 5. Hvad er sandsynligheden for at blive dødsdømt og ikke blive dødsdømt, når man er hvid? 6. Hvad er sandsynligheden for at blive dødsdømt eller ikke blive dødsdømt, når man er hvid? Svar dødsdom 1. Sandsynligheden for at blive dødsdømt, hvis man er hvid tiltalt for at have dræbt en hvid kan udregnes til: \\[\\small P(Dø\\ dsdø\\ mt \\mid hvid\\ tiltalt\\ og\\ hvidt\\ offer)=\\frac{53}{467}=0.1135\\] 2. Sandsynligheden for at blive dødsdømt, givet man er sort tiltalt for at have dræbt en hvid kan udregnes til: \\[\\small P(Dø\\ dsdø\\ mt \\mid sort\\ tiltalt\\ og\\ hvidt\\ offer)=\\frac{11}{48}=0.2292\\] 3. Sandsynligheden for at blive dødsdømt, givet man er hvid tiltalt for at have dræbt en sort kan udregnes til: \\[\\small P(Dø\\ dsdø\\ mt \\mid hvid\\ tiltalt\\ og\\ sort\\ offer)=\\frac{0}{16}=0\\] 4. Sandsynligheden for at blive dødsdømt, når man er sort tiltalt for at have dræbt en sort kan udregnes til \\[\\small P(Dø\\ dsdø\\ mt \\mid sort\\ tiltalt\\ og\\ sort\\ offer)=\\frac{4}{143}=0.0280\\] Vi ser nu at der synes at være større sandsynlighed for dødsdom for sorte end for hvide, denne effekt kommer først frem når vi kontrollerer for offer race. Man kalder dette for Simpsons paradoks. 5. Sandsynligheden for at blive dødsdømt og ikke blive dødsdømt når man er hvid, er 0, der er ingen hvide der både bliver dødsdømte og ikke dødsdømte. Fællesmængden mellem de 2 hændelser er tom. 6. Sandsynligheden for at blive dødsdømt eller ikke blive dødsdømt når man er hvid er 1, alle hvide tiltalte bliver enten dødsdømte eller ikke dødsdømte. Foreningsmængden for de 2 hændelser er alle de mulige udfald. Man kan kun blive dødsdømt eller ikke dødsdømt. Spørgsmål utroskab og skolegang Herunder er angivet tro og utro personer delt på Uddannelsesniveau. Data stammer fra en kendt amerikansk undersøgelse om utroskab fra 1969. Man har interviewet 601 respondenter om religiøsitet, antal affærer uddannelsesniveau etc. Uddannelseniveauet er stigende, hvor Grundskole er lavest og Phd højest. KVU betyder kortere videregående uddannelse. Uddannelsesniveau Basis Gymnasie KVU1 KVU2 KVU3 Master Phd. Total Antal utro 5 31 119 95 62 79 60 451 Antal tro 2 13 35 20 27 33 20 150 Total 7 44 154 115 89 112 80 601 1. Hvad er sandsynligheden for at man er utro? 2. Hvad er sandsynligheden for man er tro? 3. Hvad er sandsynligheden for at man er utro eller tro? 4. Hvad er sandsynligheden for at man er utro og tro? 5. Hvad er sandsynligheden for at man er utro, givet man har en Phd? 6. Hvad er sandsynligheden for at man har en Phd? 7. Hvad er sandsynligheden for at man har en Phd eller Kandidatgrad og er tro? 8. Hvad er sandsynligheden for at man har en Phd, når man er tro? 9. Hvad er sandsynligheden for at man har en Phd eller Kandidatgrad, når man er tro? 10. Hvad er sandsynligheden for at man har en Phd og Kandidatgrad, givet man er tro? 11. Når man er utro, hvad er da sandsynligheden for, at man har en Kandidatgrad? 12. Når man er tro, hvad er da sandsynligheden for, at man har en Phd? "],
["chi-i-anden-tests.html", "Kapitel 2 Chi i anden tests 2.1 Goodness of fit test 2.2 Forudsætning 2.3 Chi i anden test 2.4 Selvtest", " Kapitel 2 Chi i anden tests 2.1 Goodness of fit test .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Goodnees of fit testen er en udvidelse af z-testet for en andel. Med test af andele kan man fx. undersøge om andelen af mænd er 60% og kvinder 40% i en population, vi tester altså fordelingen for en kvalitativ variabel med 2 mulige udfald. Med et goodness of fit test kan vi teste kvalitative variable med 2 eller flere mulige udfald, man kan fx. undersøge om fordelingen af boligform i en stikprøve kan antages at svare til fordelingen på regionsplan: 50% ejer, 20% andel og 30% leje. Vi tester vha. Chi i anden fordelingen. Teststørrelsen vi finder, udtrykker forskellen mellem det vi observerer i stikprøven og det vi tester under nulhypotesen. Antag man simpelt tilfældigt har udtaget en stikprøve på 150 boliger, der indeholder 60 ejer- 40 andels- og 50 lejeboliger. Hvis vi vil undersøge undersøge om fordelingen af boligform i stikprøven, kan antages at følge regionsfordelingen som er 50% ejer, 20% andel og 30% leje, opstiller vi følgende hypoteser: \\[H_0:p_{ejer}=0.5\\ p_{andel}=0.2\\ p_{leje}=0.3\\]\\[H_1:Fordelingen\\ af\\ boliger\\ fø\\ lger\\ ikke\\ samme\\ fordeling\\ som\\ i\\ regionen\\] Teststørrelsen findes som: \\[\\chi^2=\\sum^k_{j=1}\\frac{(O-E)^2}{E}\\] Hvor O er observerede værdier og E er forventede værdier det stammer fra expected på engelsk, k angiver antallet af mulige udfald for den kvalitative variabel. For at beregne teststørrelsen bestemmer vi E, antallet af ejer, leje og andel vi ville forvente i en stikprøve på netop 150 boliger, der perfekt repræsenterede regionen. ejer: \\(0.5\\cdot150=75\\) andel: \\(0.2\\cdot150=30\\) leje: \\(0.3\\cdot150=45\\) Vi kan nu udregne teststørrelsen som: \\[\\chi^2=\\frac{(60-75)^2}{75}+\\frac{(40-30)^2}{30}+\\frac{(50-45)^2}{45}=3+3\\frac{1}{3}+\\frac{5}{9}=6.8889\\] Vi sammenligner med chi i anden fordelingen med k-1=3-1=2 frihedsgrader \\(\\chi^2_2\\), den kritiske værdi bliver 5.9915 hvilket giver p-værdien 0.0319, illustreret ved den gule hale i figuren nedenfor. Da teststørrelsen 6.89 er større end den kritiske værdi 5.99, får vi en p-værdi der er mindre end 5% signifikanssandsynligheden. Vi forkaster nulhypotesen og konkluderer, fordelingen af boligtyper i populationen, er ikke identiske med fordelingen i regionen. I Freestat tastes input i de hvide felter, hvilket resulterer i følgende resultat: 2.2 Forudsætning En forudsætning for at goodness of fit testet er tilstrækkeligt præcist, er at de forventede værdier E er tilstrækkeligt store. Der er mange forskellige tolkninger, af størrelsen af E cellerne. Nogle nævner celleværdier skal være større end 3 andre 5, det bør under alle omstændigheder nævnes om forudsætningen synes opfyldt. Hvis de forventede værdier er meget små, kan man sammenlægge kategorier, der vil så være et tradeoff med detaljegraden af analysen. Hvis man sammenlægger bør man gøre dette, så det analytisk giver mening. I eksemplet med boligtyper, havde vi forventede værdier E på hhv. 75, 30 og 45, her var forudsætningen altså opfyldt. Spørgsmål datasæt karakterer Undervisningsministeriet har et ønske om at karaktererne på landsplan bør normaliseres omkring 7, hvor der er følgende procentvise vægt på hver karakter Karakter Ønsket fordeling 02 10% 4 25% 7 30% 10 25% 12 10% Der er intet krav til andelen af studerende der består, således drejer fordelingen sig udelukkende om bestået-karakterer. Hent datasættet statkarakterer for stikprøven for statistikstuderende , betragt kun de beståede studerende, kan populationen antages at følge de generelle retningslinjer? Svar datasæt karakterer Vi starter med at se på de beståede 37 studerende, optæl fx. vha. =countif eller =tælhvis i excel for at bestemme antallet af studerende med de respektive karakterer. Karakter Ønsket fordeling Observeret antal Observeret Frekvens 02 10% 9 0.2432 4 25% 6 0.1622 7 30% 5 0.1351 10 25% 12 0.3243 12 10% 5 0.1351 Vi kan nu bestemme den forventede karakterfordeling hvis karaktererne følger den ønskede fordeling. Karakter Ønsket fordeling Forventet antal Chi i anden bidrag 02 10% 3.7 7.5919 4 25% 9.25 1.1419 7 30% 11.1 3.3523 10 25% 9.25 0.8176 12 10% 3.7 0.4568 Bemærk forventede værdier er mindre end 5 men større end 3, der kan være problemer med præcisionen. Hvis man ønsker at sammenlægge kategorier giver det ikke mening at lægge 02 og 12 sammen, men gerne 02 og 4 eller 10 og 12. Summen af chi i anden bidrag giver teststørrelsen, dvs: 7.5919+1.1419+3.3523+0.8176=13.3604 Hvilket fører til p-værdien 0.0096 illustreret ved den gule hale herunder, da p-værdien er mindre end 5% signifikansniveauet forkaster vi nulhypotesen, og konkluderer at statistikkarakterer på Finansøkonom ikke følger den ønskede fordeling. Vi kan ud fra chi i anden bidragene se hvilke karakterer der giver de største afvigelser. Store bidrag betyder store afvigelser mellem det observede og ønskede. Det største bidrag 7.5919 stammer fra 02 karakteren, her er den observerede karakter 9, mens den forventede værdi er 3.7. Der er altså flere studerende, end forventet der får 02. Bemærk for at vi kan udtale os om populationen finansøkonomer, fordres at stikprøven er repræsentativ for finansøkonomer. Stikprøven er ikke udtaget simpelt tilfældigt, da der er tale om 2 bestemte klasser, det kan derfor diskuteres om stikprøven er afspejler populationen korrekt. Freestat output bliver: 2.3 Chi i anden test .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } ## Chi i anden test 2 .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Vi kan analysere kvalitative variable med 2 mulige udfald vha. test af 2 andele. Chi i anden testet er en udvidelse af test af 2 andele. Med chi i anden testen kan man sammenligne kvalitative variable med 2 eller flere mulige udfald. Vi kan benytte chi i anden testet til at undersøge om der er en sammenhæng mellem 2 inddelingskriterier som fx. køn og bestået/ikke bestået, køn og karakter, aldersgruppe og karakter. Antag et forsikringsselskab har indsamlet data for kunders skadesanmeldelser fordelt på øst og vest for Storebælt. Forsikringsselskabet ønsker at undersøge om der er forskel i andelen af kunder der anmelder skader i Øst- og Vestdanmark. Følgende data er angivet Observeret Ingen skader anmeldt 1 eller flere skader Total Østdanmark 300 300 600 Vestdanmark 250 150 400 Total 550 450 1000 Vi kan teste om der er forskel på om der er forskel på andelen af anmeldte skader i Øst- og Vestdanmark vha. chi i anden testet. Vi har følgende hypoteser. \\[H_0: Der\\ er\\ uafhæ\\ \\ ngighed\\ mellem\\ ræ\\ kke-\\ og\\ sø\\ jlekriterierne\\]\\[H_1: Der\\ er\\ afhæ\\ \\ ngighed\\ mellem\\ ræ\\ kke-\\ og\\ sø\\ jlekriterierne\\] Eller mere præcist i dette tilfælde: \\[H_0: Der\\ er\\ uafhæ\\ \\ ngighed\\ mellem\\ landsdel\\ og\\ skadesanmeldelse\\]\\[H_1: Der\\ er\\ afhæ\\ \\ ngighed\\ mellem\\ landsdel\\ og\\ skadesanmeldelse\\] Hvis nulhypotesen forkastes påvirker landsdelen kunder kommer fra altså andelen af anmeldte skader. 2.3.1 Uafhængighed Definitionen af uafhængighed mellem 2 hændelser A og B er at sandsynligheden for fælleshændelsen er lig med produktet af sandsynlighederne for enkelthændelserne som formel skriver vi: \\[P(A\\cap B)=P(A)\\cdot P(B)\\] Vores hændelse A kan fx. være kunden stammer fra Østdanmark, og hændelse B at kunden har ikke anmeldt skader. Vi får da følgende ligning: \\[P(Ø\\ \\ stdanmark\\cap 0\\ skader)=P(Ø\\ \\ stdanmark)\\cdot P(0\\ skader)\\] Vi kan omskrive dette til: \\[P(Ø\\ \\ stdanmark\\cap0\\ skader)=P(Ø\\ \\ stdanmark)\\cdot P(0\\ skader)\\Leftrightarrow \\frac{300}{1000}=\\frac{600}{1000}\\cdot \\frac{550}{1000} \\Leftrightarrow \\]\\[1000\\cdot\\frac{300}{1000}=1000\\cdot \\frac{600\\cdot550}{1000\\cdot1000} \\Leftrightarrow 300= \\frac{600\\cdot550}{1000}\\] Her er venstresiden i ligningen jo den observerede celleværdi. Hvis der er uafhængighed under nulhypotesen, vil vi forvente at den observerede værdi, er lig med venstresiden, som vi kalder den forventede værdi. Hvis der er perfekt uafhængighed mellem landsdel og skadesanmeldelse, ville vi altså i hver celle forvente værdien: \\[\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}\\] Vi får derfor følgende matrice. Forventet Ingen skader anmeldt 1 eller flere skader Total Østdanmark \\(\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}=\\frac{600\\cdot 550}{1000}=330\\) \\(\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}=\\frac{600\\cdot 450}{1000}=270\\) 600 Vestdanmark \\(\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}=\\frac{400\\cdot 550}{1000}=220\\) \\(\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}=\\frac{400\\cdot 450}{1000}=180\\) 400 Total 550 450 1000 Vi kan nu beregne chi i anden cellebidragene med samme formel som for goodness of fit testet: \\[\\frac{(O-E)^2}{E}\\] Chi celle bidrag Ingen skader anmeldt 1 eller flere skader Total Østdanmark \\(\\frac{(300-330)^2}{330}=2.7272727\\) \\(\\frac{(300-270)^2}{270}=3.3333333\\) Vestdanmark \\(\\frac{(250-220)^2}{220}=4.0909091\\) \\(\\frac{(150-180)^2}{180}=5\\) Total 15.15 Teststørrelsen bliver 15.15, denne bruger vi til at beregne p-værdien for testet af uafhængighed. Antallet af frihedsgrader for chi i anden fordelingen er antallet af rækkeinddelingskriterier Østdanmark og Vestdanmark minus 1, gange antallet af søjleinddelingskriterier 0 skader og flere end 0 skader minus 1, dvs. \\[(r-1)\\cdot(s-1)=(2-1)\\cdot(2-1)=1\\cdot1=1\\] Vi får p-værdien 9.910^{-5}, hvilket er klart mindre end signifikansniveauet på 5%, arealet er så lille vi ikke kan se det på figuren nedenfor. Vi forkaster altså nulhypotesen og konstaterer der er afhængighed mellem landsdel og anmeldte skader. Landsdelen som kunden stammer fra, påvirker altså antallet af anmeldte skader. Vi kan nu se om der er chi i anden bidrag, der er meget store og dermed bidrager stæ til konklusionen om afhængighed. Der er ikke en voldsom forskel i størrelserne på chi i anden bidragene, men når vi ser på observeret mod forventet, ser vi at 150 anmelder skader, det var forventet at 180 personer fra Vestdanmark anmelder skader. Denne tendens er modsat for Østdanmark. Vestdanmark anmelder altså færre skader end Østdanmark. Ligesom for goodness of fit testet, skal de forventede værdier have en vis størrelse for at vore konklusioner er præcise. Forudsætningen om forventede værdier større end 5 er opfyldt for alle celler. Freestat output bliver 2.3.2 Anmeldte skader fordelt på regioner og antal skader Vi antager nu der foreligger mere specifikke data for undersøgelsen omkring geografisk placering og skadesanmeldelse.Vi har finere inddeling på region og antal skader. Observeret 0 skader 1 skade 2 eller flere skader Total Hovedstaden 150 125 50 325 Sjælland 150 100 25 275 Syddanmark 75 30 10 115 Midtjylland 75 40 10 125 Nordjylland 100 45 15 160 Total 550 340 110 1000 Vi kan teste om der er forskel på om der er forskel på andelen af anmeldte skader i Øst- og Vestdanmark vha. chi i anden testet. Vi har følgende hypoteser. \\[H_0: Der\\ er\\ uafhæ\\ \\ ngighed\\ mellem\\ region\\ og\\ antal skader\\]\\[H_1: Der\\ er\\ afhæ\\ \\ ngighed\\ mellem\\ region\\ og\\ antal skader\\] Hvis nulhypotesen forkastes påvirker regionen kunder kommer fra altså antallet af anmeldte skader. Vi beregner de forventede værdier efter den sædvanlige formel: \\[\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}\\] Hvilket giver følgende matrix Forventet 0 skader 1 skade 2 eller flere skader Total Hovedstaden 178.75 110.5 35.75 325 Sjælland 151.25 93.5 30.25 275 Syddanmark 63.25 39.1 12.65 115 Midtjylland 68.75 42.5 13.75 125 Nordjylland 88 54.4 17.6 160 Total 550 340 110 1000 Vi kan nu beregne chi i anden cellebidragene med samme formel som for goodness of fit testet: \\[\\frac{(O-E)^2}{E}\\] Chi celle bidrag 0 skader 1 skade 2 eller flere skader Total Hovedstaden 4.6241259 1.9027149 5.6800699 12.2069107 Sjælland 0.0103306 0.4518717 0.911157 1.3733593 Syddanmark 2.1828063 2.1179028 0.5551383 4.8558475 Midtjylland 0.5681818 0.1470588 1.0227273 1.7379679 Nordjylland 1.6363636 1.6242647 0.3840909 3.6447193 Total 9.0218082 6.2438129 8.5531835 23.8188046 Teststørrelsen bliver 23.82, denne bruger vi til at beregne p-værdien for testet af uafhængighed. Antallet af frihedsgrader bliver \\[(r-1)\\cdot(s-1)=(5-1)\\cdot(3-1)=4\\cdot 2=8\\] Vi får p-værdien 0.002458, hvilket er klart mindre end signifikansniveauet på 5%. Vi forkaster nulhypotesen og konstaterer, der er afhængighed mellem region og antal anmeldte skader. Regionen som kunden stammer fra, påvirker altså antallet af anmeldte skader. Vi kan se, der er chi i anden bidrag, der er store for region København, disse bidrager kraftigt til konklusionen om afhængighed. Københavnerne anmelder flere skader end forventet, dermed er der færre københavnere end forventet, der ikke anmelder skader. Forudsætningen om forventede værdier større end 5 er opfyldt for alle celler. Freestat output bliver Spørgsmål Titanic I 1912 forliste Titanic, vi har i filen oplysninger om passagererne. Har man har større chance for at overleve, hvis man er velhavende? Vi har ikke oplysninger om passagerernes formuer, men vi kan bruge oplysningerne om billetterne som en proxy for velstand. Variablen pclass angiver hvilken billet den pågældende passager havde, 1. klasse er dyrest. Variablen survived fortæller om en passager overlevede 1 eller døde 0. Data er i filen Titanic. Svar Titanic Vi sorterer passagerer efter billet og om de har overlevet. Observeret Døde Overlevede Total 1. Klasse 123 200 323 2. Klasse 158 119 277 3. Klasse 528 181 709 Total 809 500 1309 Vi kan teste om der er billettype betyder noget for overlevelse. Vi får følgende hypoteser: \\[H_0: Der\\ er\\ uafhæ\\ \\ ngighed\\ mellem\\ passagerklasse\\ og\\ overlevelse\\]\\[H_1: Der\\ er\\ afhæ\\ \\ ngighed\\ mellem\\ passagerklasse\\ og\\ overlevelse\\] Hvis nulhypotesen forkastes betyder passagerklasse noget for noget for overlevelsen Vi beregner de forventede værdier: \\[\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}\\] Hvilket giver følgende matrix Forventet Døde Overlevede Total 1. Klasse 199.62 123.38 323 2. Klasse 171.19 105.81 277 3. Klasse 438.18 270.82 709 Total 809 500 1309 Vi kan nu beregne chi i anden cellebidragene med samme formel som for goodness of fit testet: \\[\\frac{(O-E)^2}{E}\\] Chi celle bidrag Døde Overlevede Total 1. Klasse 29.4111 47.5871 76.9982 2. Klasse 1.0169 1.6453 2.6622 3. Klasse 18.4105 29.7882 48.1987 Total 48.8385 79.0207 127.8592 Teststørrelsen bliver 127.86, denne bruger vi til at beregne p-værdien for testet af uafhængighed. Antallet af frihedsgrader bliver \\[(r-1)\\cdot(s-1)=(3-1)\\cdot(2-1)=2\\cdot 1=2\\] Vi får p-værdien 0, hvilket er klart mindre end signifikansniveauet på 5%. Vi forkaster nulhypotesen og konstaterer, der er afhængighed mellem passagerklasse og overlevelse. Forudsætningen om forventede værdier større end 5 er opfyldt for alle celler. Vi kan se at 200 1. klasses passagerer overlevede mod forventet 123.38 under nulhypotesen, hvilket giver et meget stort chi i anden bidrag. Omvendt overlevede kun 181 3. klasses passagerer mod 270.82 forventet under nulhypotesen. Der var altså væstentlig større chance for overlevelse hvis man er velhavende. Spørgsmål bankansatte Vi ser på data for bankansatte i filen Bankdata filen. Er der sammenhæng mellem jobfunktion og køn? Svar bankansatte Vi sorterer personalet efter jobfunktion og køn. Observeret Kvinde Mand Total Administration 206 157 363 Sikkerhedspersonale 0 27 27 Ledelse 10 74 84 Total 216 258 474 Vi kan teste om der er billettype betyder noget for overlevelse. Vi får følgende hypoteser: \\[H_0: Der\\ er\\ uafhæ\\ \\ ngighed\\ mellem\\ jobfunktion\\ og\\ kø\\ n\\]\\[H_1: Der\\ er\\ afhæ\\ \\ ngighed\\ mellem\\ jobfunktion\\ og\\ kø\\ n\\] Hvis nulhypotesen forkastes har køn betydning for jobfunktion. Vi beregner de forventede værdier: \\[\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}\\] Hvilket giver følgende matrix Forventet Kvinde Mand Total Administration 165.42 197.58 363 Sikkerhedspersonale 12.3 14.7 27 Ledelse 38.28 45.72 84 Total 216 258 474 Vi kan nu beregne chi i anden cellebidragene med samme formel som for goodness of fit testet: \\[\\frac{(O-E)^2}{E}\\] Chi celle bidrag Kvinde Mand Total Administration 9.9561 8.3354 18.2915 Sikkerhedspersonale 12.3038 10.3009 22.6047 Ledelse 20.8909 17.4901 38.381 Total 43.1508 36.1264 79.2772 Teststørrelsen bliver 79.28, denne bruger vi til at beregne p-værdien for testet af uafhængighed. Antallet af frihedsgrader bliver \\[(r-1)\\cdot(s-1)=(3-1)\\cdot(2-1)=2\\cdot 1=2\\] Vi får en meeget lille p-værdi afrundet til 0, hvilket er klart mindre end signifikansniveauet på 5%. Vi forkaster nulhypotesen og konstaterer, der er afhængighed mellem jobfunktion og køn. Forudsætningen om forventede værdier større end 5 er opfyldt for alle celler. Udfra tabellerne ses at mænd er underrepræsenteret i administrationen og overrepræsenteret i sikkerhedspersonale og ledelse. Spørgsmål bankansatte Vi ser fortsat på data for bankansatte i filen Bankdata filen. Er der sammenhæng mellem jobfunktion og minoritet? Minoritet er ikke-hvide. Svar bankansatte Vi sorterer personalet efter jobfunktion og køn. Observeret Ikke-minoritet minoritet Total Administration 276 87 363 Sikkerhedspersonale 14 13 27 Ledelse 80 4 84 Total 370 104 474 Vi kan teste om minoritet betyder noget for jobfunktion. Vi får følgende hypoteser: \\[H_0: Der\\ er\\ uafhæ\\ \\ ngighed\\ mellem\\ jobfunktion\\ og\\ minoritet\\]\\[H_1: Der\\ er\\ afhæ\\ \\ ngighed\\ mellem\\ jobfunktion\\ og\\ minoritet\\] Hvis nulhypotesen forkastes betyder det at tilhører man en minoritet har dette betydning for jobfunktionen. Vi beregner de forventede værdier: \\[\\frac{ræ\\ \\ kkesum\\cdot sø\\ jlesum}{totalsum}\\] Hvilket giver følgende matrix Forventet Ikke-minoritet Minoritet Total Administration 283.35 79.65 363 Sikkerhedspersonale 21.08 5.92 27 Ledelse 65.57 18.43 84 Total 370 104 474 Vi kan nu beregne chi i anden cellebidragene med samme formel som for goodness of fit testet: \\[\\frac{(O-E)^2}{E}\\] Chi celle bidrag Ikke-minoritet minoritet Total Administration 0.1909 0.6791 0.87 Sikkerhedspersonale 2.3756 8.4518 10.8274 Ledelse 3.1758 11.2985 14.4743 Total 5.7423 20.4294 26.1717 Teststørrelsen bliver 26.17, denne bruger vi til at beregne p-værdien for testet af uafhængighed. Antallet af frihedsgrader bliver \\[(r-1)\\cdot(s-1)=(3-1)\\cdot(2-1)=2\\cdot 1=2\\] Vi får en lille p-værdi på 210^{-6}, hvilket er klart mindre end signifikansniveauet på 5%. Vi forkaster nulhypotesen og konstaterer, der er afhængighed mellem jobfunktion og om man tilhører en minoritet. Forudsætningen om forventede værdier større end 5 er opfyldt for alle celler. Udfra tabellerne ses at minoriteter er overrepræsenteret blandt administration og sikkerhedspersonale og underrepræsenteret i ledelsen. 2.4 Selvtest Selvtest Chi i anden med videoløsninger "],
["anova.html", "Kapitel 3 ANOVA 3.1 Selvtest", " Kapitel 3 ANOVA .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } ANOVA er en metode til at sammenligne middelværdierne for mere end to kvantitative variable. Skal man sammenligne to middelværdier med varianshomogenitet, benytter man pooled t-test, er der mere end 2 middelværdier benyttes ANOVA F-test . Forudsætningerne for at benytte testen er at populationerne er normalfordelte og har samme varians. ANOVA er en forkortelse af analysis of variances, man tester om middelværdierne er ens vha. varianserne. Vi undersøger om k populationer har samme middelværdi, hypoteserne bliver: \\[H_0:\\mu_1=\\mu_2=...=\\mu_k\\]\\[H_1:Ikke\\ alle\\ middelvæ\\ rdier\\ er\\ ens.\\] Den totale variation SST kan opdeles i SSW og SSA hvor, SSW er variationen indenfor de k grupper, SSA er variationen mellem grupperne. Hvis variationen indenfor grupperne SSW er lille i forhold til variationen mellem grupperne SSA, er middelværdierne ikke ens. Herunder er et eksempel, hvor populationerne er dagsafkast for aktier, variationen indenfor grupperne SSW er lille i forhold til variationen mellem grupperne SSA, derfor er middelværdierne signifikant forskellige. Herunder er en figur med dagsafkast for aktier, hvor variationen indenfor grupperne SSW er stor i forhold til variationen mellem grupperne SSA, derfor er middelværdierne ikke signifikant forskellige. Herunder er et eksempel hvor forudsætningen om varianshomogenitet ikke er opfyldt, de 3 aktier har forskelligt variation. Et forsikringsselskab har udviklet 4 forskellige layouts til information om skadesdækning. Brugerne udsættes vilkårligt for et af de 4 layouts, selskabet registrerer tiderne for besøgene på hjemmesiderne for at afgøre hvilket design, der er optimalt mht. brugervenlighed og overskuelighed. .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Hent datasættet Hjemmesidedesigns besøgstider i millisekunder, der viser de 359 observede besøgstider på de 4 hjemmesider. Forsikringsselskabet ønsker at undersøge om der er forskel på besøgstiderne, vi opstiller hypoteserne: \\[H_0:\\mu_{Design 1}=\\mu_{Design 2}=\\mu_{Design 3}=\\mu_{Design 4}\\]\\[H_1:Ikke\\ alle\\ middelvæ\\ rdier\\ er\\ ens\\ for\\ de\\ 4\\ designs.\\] Freestat output Vi får en F-teststørrelse på 6.0429, der resulterer i en p-værdi på 0.0005, hvilket er under signifikansniveauet på 0.05. Vi kan forkaster altså nulhypotesen om ens middelværdier. Freestat output Vi skal tjekke forudsætningen om varianshomogenitet \\[H_0:\\sigma_{Design 1}=\\sigma_{Design 2}=\\sigma_{Design 3}=\\sigma_{Design 4}\\]\\[H_1:Ikke\\ alle\\ varianser\\ er\\ ens\\ for\\ de\\ 4\\ designs.\\] Vi får en teststørrelse på 1.4742741. Chi i anden testet giver os en p-værdi på 0.6882205, hvilket er større end signifikanssandsynligheden på 0.05, vi kan ikke afvise nulhypotesen. Varianserne er ens, så forudsætningen er opfyldt. Freestat output af Bartlett test for varianshomogenitet Normalitet Herunder er 4 normalfraktildiagrammer, for de 4 designs, vi kan godt antage, stikprøverne stammer fra normalfordelte populationer. Tukey Kramer Vi kan undersøge hvilke designs der har de største afvigelser ved at se på forskellene mellem stikprøvegennemsnittene, der er størst forskel mellem besøgstiderne for design 1 og design 4. Vi kan grafisk sammenligne middelværdierne i boxplots, her ser vi ligeledes forskellen er størst mellem design 1 og design 4. Middelværdierne er markeret med orange prikker. Spørgsmål 3 banker afkast Hent datasættet 3 Danske Banker Ugeafkast i procent , i dette datasæt er de seneste 249 ugers afkast i procent for hhv. Danske Bank, Jyske Bank og Sydbank. Er der signifikant forskel på afkastene på de 3 bankaktier? Svar 3 banker afkast Vi opstiller hypoteserne for test af om middelværdierne er identiske: \\[H_0:\\mu_{Danske Bank}=\\mu_{Jyske Bank}=\\mu_{Sydbank}\\]\\[H_1:Ikke\\ alle\\ middelvæ\\ rdier\\ er\\ ens\\ for\\ de\\ 3\\ banker\\] Vi får variationen i grupperne SSW til 6792.4509 og variationen mellem grupperne SSA til 3.0922. Dette giver en F-teststørrelse på 0.1693 der resulterer i p-værdi på 0.8442. Vi kan altså ikke forkaste altså nulhypotesen om ens middelværdier. Vi kan da se at de absolutte forskelle mellem stikprøvegennemsnittene er relativt små: Forskelle mellem Banker Absolutte forskelle Danske Bank - Jyske Bank 0.1 Danske Bank - Sydbank 0.05 Jyske Bank - Sydbank 0.15 Vi skal tjekke forudsætningen om varianshomogenitet \\[H_0:\\sigma_{Danske Bank}=\\sigma_{Jyske Bank}=\\sigma_{Sydbank}\\]\\[H_1:Ikke\\ alle\\ varianser\\ er\\ ens\\ for\\ de\\ 3\\ banker.\\] Vi får en teststørrelse på 6.3564787. Chi i anden testet giver os en p-værdi på 0.0416589, hvilket er mindre end signifikanssandsynligheden på 0.05, vi afviser nulhypotesen. Varianserne er ikke ens, så forudsætningen er ikke opfyldt. Vi har således problemer med kvaliteten af analysen. Normalitet Vi kan grafisk sammenligne middelværdierne for ugeafkastet for de 3 banker i boxplots , her ser vi der ikke er stor forskel på middelværdierne. Middelværdierne er markeret med orange prikker. Spørgsmål IMDB Hent IMDB data, der viser data for 759 film simpelt tilfældigt udtrukket af en database med 759 film. Vi ønsker at se om, der er forskel på vurderingen af de forskellige genrer action, komedie, drama, documentar, romance og short, undersøg dette vha. ANOVA test Svar IMDB Vi opstiller hypoteserne: \\[H_0:\\mu_{action}=\\mu_{komedie}=\\mu_{drama}=\\mu_{documentar}=\\mu_{romance}=\\mu_{short}\\]\\[H_1:Ikke\\ alle\\ middelvæ\\ rdier\\ er\\ ens\\ for\\ de\\ 5\\ genrer.\\] Vi får en F-teststørrelse på 10.786, der resulterer i en meget lille p-værdi på 0, hvilket er klart under signifikansniveauet på 0.05. Vi kan forkaster altså nulhypotesen om ens middelværdier. Vi skal tjekke forudsætningen om varianshomogenitet \\[H_0:\\sigma_{action}=\\sigma_{komedie}=\\sigma_{drama}=\\sigma_{documentar}=\\sigma_{romance}=\\sigma_{short}\\]\\[H_1:Ikke\\ alle\\ standardafvigelser\\ er\\ ens\\ for\\ de\\ 5\\ genrer.\\] Vi får en teststørrelse på 7.1306596. Chi i anden testet giver os en p-værdi på 0.2111029. Normalitet Herunder er 5 normalfraktildiagrammer, for de 5 genrer, dokumentar og short genrerne ser ikke normalfordelte ud. Hvilket kan give problemer med kvaliteten i vor analyse. Vi kan ud fra boxplots se at dokumentarfilm rates højt i modsætning til actionfilm. Vi kan i tabellen herunder se hvor de største forskelle er mellem genrerne. Forskelle gennemsnit genrer Absolutte forskelle Action - dokumentar 1.5465517 Action - drama 0.9324561 Action - komedie 0.3148402 Action - romance 0.6735294 Action - short 0.788806 Dokumentar - drama 0.6140956 Dokumentar - komedie 1.2317115 Dokumentar - romance 0.8730223 Dokumentar - short 0.7577458 Drama - komedie 0.617616 Drama - romance 0.2589267 Drama - short 0.1436502 Komedie - romance 0.3586892 Komedie - short 0.4739658 Romance - short 0.1152766 3.1 Selvtest Selvtest Anova analyse med videoløsninger "],
["cran-r.html", "Kapitel 4 Cran R 4.1 R video-tutorials", " Kapitel 4 Cran R Vi bruger software programmet R da det er gratis og kan bruges til alt indenfor statistik, det er svært i begyndelsen, så sørg for at komme til timerne :O) Der er et hav af videoer på youtube og hjælpesider fx. https://youtu.be/cX532N_XLIs?list=PLqzoL9-eJTNBDdKgJgJzaQcY6OXmsXAHU https://youtu.be/UYclmg1_KLk?list=PLqzoL9-eJTNBDdKgJgJzaQcY6OXmsXAHU https://youtu.be/2TcPAZOyV0U?list=PLqzoL9-eJTNBDdKgJgJzaQcY6OXmsXAHU https://youtu.be/qPk0YEKhqB8?list=PLqzoL9-eJTNBDdKgJgJzaQcY6OXmsXAHU https://youtu.be/3RWb5U3X-T8?list=PLqzoL9-eJTNBDdKgJgJzaQcY6OXmsXAHU https://www.datacamp.com/courses/free-introduction-to-r https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf http://dss.princeton.edu/training/RStudio101.pdf https://youtu.be/uwlwNRbaKMI 4.1 R video-tutorials Herunder er 3 begynder videotutorials forsøg at gøre det samme som i videoerne. Du kan downloade scripts fra hver tutorial direkte under hver videovindue. .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Hent r script klik her. .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Hent r script klik her. .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Hent r script klik her. "],
["faktoranalyse.html", "Kapitel 5 Faktoranalyse 5.1 Hvilke faktorer er vigtige når man køber en ny computer: 5.2 Forudsætninger for faktoranalysen. 5.3 Selve Faktoranalysen 5.4 Hvor mange faktorer bør benyttes?", " Kapitel 5 Faktoranalyse .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Faktoranalyse FA er ligesom, klyngeanalyse vi ser på senere, en strukturanalyse, der belyser hvilke sammenhænge der er i et datasæt. FA er en statistikmodel der navnlig bruges til at forenkle tolkningen af et datamateriale, der indeholder en stor mængde variable. FA bygger på et kæmpe antal udregninger med udgangspunkt i korrelationskoefficienter og er i praksis nærmest umulig at gennemføre uden brug af computersoftware. Før computerens opfindelse kunne en faktoranalyse nemt lægge beslag på en halv snes statistikere på fuld tid gennem flere måneder. Resultatet ville være identifikation få faktorer, skjult i et datamateriale. Faktoranalysen kan fx. hjælpe med at vise sammenhænge mellem svar i spørgeskemaer, hvilket kan være en hjælp markedsføringsmæssigt, til at forstå hvilke svar samvarierer. En sådan gruppering af spørgsmål kan hjælpe med bedre at forstå kundernes ønsker og behov. Faktoranalyse FA er et godt værktøj til at forstå, hvad dine spørgeskemadata betyder, især når du har mange variable. Faktoranalysen forsøger at finde skjulte variable, som forklarer opførslen af dine observerede variable. FA har historiske rødder i psykometri, dvs målingen af mentale egenskaber. I modsætning hertil søger man med Klyngeanalysen at finde sammenhænge mellem respondenterne/observationerne og altså ikke variablene, hvilket er et godt hjælpeværktøj i forbindelse med markedssegmentering. Der findes 2 typer af FA Exploratory Factor Analysis (EFA) og Confirmatory Factor Analysis (CFA). EFA betyder, at du ikke rigtig ved, hvilke skjulte variable (eller faktorer) der findes, og hvor mange de er, så du forsøger at finde dem. CFA betyder, at du allerede har nogle gæt eller modeller på skjulte variable (eller faktorer), og vi vil kontrollere, om dette er korrekt. Vi benytter i det følgende EFA 5.1 Hvilke faktorer er vigtige når man køber en ny computer: .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Lad os sige, man har indsamlet et spørgeskema for at undersøge, hvad der er vigtigt, når en forbruger bestemmer, hvilken computer der skal købes. Spørgeskemaet er udformet, som hvor vigtig er Pris for computerkøbet, vægt fra 1 til 7 hvor 7 er højest/vigtigst. Dette datasæt er lille i forhold til, hvad der er realistisk, da vi blot forsøger at illustrere analysen. Normalt vil man have flere end 6 variable. For at sikre solid analyseresultater vil man have flere respondenter/observationer. I noterne er input til R og output fra R, markeret i grå rammer. Output dvs. resultaterne R leverer er markeret med ## i hver linje. Input dvs. de kommandoer vi skal skrive ind i R er ikke markeret. Rammen herunder er input til R, da linjerne ikke er markeret. Læs datasættet ind i R, dette kan du gøre ved at copy paste nedenstående kode, i den grå ramme, direkte ind i R enten i console eller et script. Bemærk data.frame er en to-dimensionel data struktur, nedenstående betyder vore 6 variable nu er lagret i en dataframe. Pris &lt;- c(6,7,6,5,7,6,5,6,3,1,2,5,2,3,1,2) Software &lt;- c(5,3,4,7,7,4,7,5,5,3,6,7,4,5,6,3) Æstetik &lt;- c(3,2,4,1,5,2,2,4,6,7,6,7,5,6,5,7) Brand &lt;- c(4,2,5,3,5,3,1,4,7,5,7,6,6,5,5,7) Venner &lt;- c(7,2,5,6,2,4,1,7,3,2,6,7,6,2,4,5) Familie &lt;- c(6,3,4,7,1,5,4,5,4,4,5,7,2,3,5,6) data &lt;- data.frame(Pris, Software, Æstetik, Brand, Venner, Familie) 5.2 Forudsætninger for faktoranalysen. .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Nu skal vi undersøge om det er fornuftigt at foretage en faktoranalyse, dette kan vi undersøge ved enten Bartletts korrelationstest eller Kaiser-Meyer-Olkin KMO. For at teste dette skal man installere og loade pakken psych, vi skal kun installere første gang. Bemærk install.packages(“psych”) er kommenteret ud nedenfor, så første gang skal du kommentere install.packages(“psych”) ud. #install.packages(&quot;psych&quot;) library(&quot;psych&quot;) Du kan alternativt benytte pacman pakken, hvilket ofte er at foretrække. Hvis du indlæser nedenstående installeres og loades pakken psych: if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(psych) Nu hvor psych pakken er indlæst kan vi køre Bartletts test i R: cortest.bartlett(data) ## $chisq ## [1] 36.15696 ## ## $p.value ## [1] 0.00167791 ## ## $df ## [1] 15 Nulhypotesen i Bartletts test er at variablene ikke er korrelerede dvs. alle korrelationskoefficienter \\(\\rho=0\\), \\[H_0:Alle\\ \\rho=0\\] \\[H_1:Ikke\\ alle\\ \\rho=0 \\] Forkaster vi nulhypotesen, er der basis for at gennemføre faktoranalysen. Den lille p-værdi på 0.00167791, betyder vi gennemfører faktoranalysen. Vi kan ligeledes køre Kaiser-Meyer-Olkin KMO testet fra pshych pakken: KMO(data) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = data) ## Overall MSA = 0.49 ## MSA for each item = ## Pris Software Æstetik Brand Venner Familie ## 0.71 0.70 0.57 0.49 0.30 0.37 For KMO skal Overall MSA = 0.49, fortæller om egnethed af data til faktoranalyse. Skal være større end kritisk grænse på ca. 0.5, dette er ikke helt tilfældet her, men vi gennemfører alligevel faktoranalysen. Vi kan se af korrelationsmatricen, at nogle variable er korrellerede og andre tilsyneladende ikke. cor(data) ## Pris Software Æstetik Brand Venner ## Pris 1.00000000 0.1856123 -0.63202219 -0.58026680 0.03082006 ## Software 0.18561230 1.0000000 -0.14621516 -0.11858645 0.10096774 ## Æstetik -0.63202219 -0.1462152 1.00000000 0.85285436 0.03989799 ## Brand -0.58026680 -0.1185864 0.85285436 1.00000000 0.33316719 ## Venner 0.03082006 0.1009677 0.03989799 0.33316719 1.00000000 ## Familie -0.06183118 0.1765724 -0.06977360 0.02662389 0.60727189 ## Familie ## Pris -0.06183118 ## Software 0.17657236 ## Æstetik -0.06977360 ## Brand 0.02662389 ## Venner 0.60727189 ## Familie 1.00000000 Så det ser ud til, at Pris har stærke negative sammenhænge med Æstetik og Brand. Venner har en stærk positiv sammenhæng med Familie. Det betyder, at vi kan forvente, at vi vil have to fælles faktorer, og en vil være relateret til pris, æstetik og brand, og den anden vil være relateret til ven og familie. Vi kan lave et corrplot, der grafisk illustrerer disse sammenhænge, store blå prikker er positiv korrelation, store røde negativ korrelation. Så vi skal installere corplott pakken, her bruger vi pacman til installationen. pacman::p_load(corrplot) corrplot(cor(data), order = &quot;hclust&quot;, tl.col=&#39;black&#39;, tl.cex=.5) 5.3 Selve Faktoranalysen .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Vi er nu klar til at teste om der er grundlag for at sammenlægge variable til faktorer. Dette tester vi i selve faktor analysen, ved hjælp af funktionen factanal. Hvis vi samler alle spørgsmål til en samlet faktor, har vi følgende faktor analyse: fa1 &lt;- factanal(data, factor=1) fa1 ## ## Call: ## factanal(x = data, factors = 1) ## ## Uniquenesses: ## Pris Software Æstetik Brand Venner Familie ## 0.567 0.977 0.126 0.167 0.974 1.000 ## ## Loadings: ## Factor1 ## Pris -0.658 ## Software -0.152 ## Æstetik 0.935 ## Brand 0.912 ## Venner 0.161 ## Familie ## ## Factor1 ## SS loadings 2.190 ## Proportion Var 0.365 ## ## Test of the hypothesis that 1 factor is sufficient. ## The chi square statistic is 12.79 on 9 degrees of freedom. ## The p-value is 0.172 Uniqueness angiver hvor meget af variationen i en variabel, der ikke er associeret med faktoren. Jo lavere Uniqueness, des større sammenhæng til faktorerne. Loadings angiver hvordan hver variabel er vægtet for faktorerne, men også hvor stærk korrelationen er til hver faktor. Faktor 1 påvirkes således mest af Æstetik, Brand og Pris. Denne faktor kan ses af corrplottet hvor disse tre variable ses at påvirke hinanden, Pris er negativt korreleret med Æstetik og Brand. Proportion Var er et vigtigt nøgletal, der angiver andelen af variationen i data, der er forklaret af den pågældende faktor. Her forklares alså kun 36.5% af variationen i datasættet. Proportion Var findes som SS loadings divideret med antallet af variable dvs. her 6. Lad os se på hvad der sker, hvis vi deler variablene op i 2 faktorer: fa2 &lt;- factanal(data, factor=2) fa2 ## ## Call: ## factanal(x = data, factors = 2) ## ## Uniquenesses: ## Pris Software Æstetik Brand Venner Familie ## 0.559 0.960 0.126 0.080 0.005 0.609 ## ## Loadings: ## Factor1 Factor2 ## Pris -0.657 ## Software -0.161 0.119 ## Æstetik 0.933 ## Brand 0.928 0.242 ## Venner 0.100 0.992 ## Familie 0.620 ## ## Factor1 Factor2 ## SS loadings 2.207 1.453 ## Proportion Var 0.368 0.242 ## Cumulative Var 0.368 0.610 ## ## Test of the hypothesis that 2 factors are sufficient. ## The chi square statistic is 2.16 on 4 degrees of freedom. ## The p-value is 0.706 Cumulative Var 0.61 er summen af proportion var 0.368 og 0.242, og betyder at faktor 1 og faktor 2 forklarer 61% af variationen i datamaterialet. Faktor 2 forklares altså primært af Venner og Familie, dette stemmer godt overens med billedet vi så i corrplot. Nu opdeler vi variablene i 3 faktorer. fa3 &lt;- factanal(data, factor=3) fa3 ## ## Call: ## factanal(x = data, factors = 3) ## ## Uniquenesses: ## Pris Software Æstetik Brand Venner Familie ## 0.468 0.944 0.154 0.005 0.193 0.005 ## ## Loadings: ## Factor1 Factor2 Factor3 ## Pris -0.717 0.122 ## Software -0.146 0.172 ## Æstetik 0.908 -0.130 ## Brand 0.892 -0.116 0.430 ## Venner 0.440 0.781 ## Familie 0.970 0.228 ## ## Factor1 Factor2 Factor3 ## SS loadings 2.161 1.198 0.872 ## Proportion Var 0.360 0.200 0.145 ## Cumulative Var 0.360 0.560 0.705 ## ## The degrees of freedom for the model is 0 and the fit was 0.0188 Nu forklares 70.5% af variationen i datamaterialet altså ud fra de 3 faktorer. 5.4 Hvor mange faktorer bør benyttes? .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Der er ingen fast regel for hvor mange faktorer der bør benyttes. Der er flere forskelle metoder til bestemmelse af antallet af faktorer, vi viser herunder 2 metoder. 5.4.1 Eigenvalues metoden Eigenvalues udtrykker hvor meget af datamaterialets samlede varians, der dækkes af den pågældende faktor. Eigenvalues er standardiserede således at summen giver antallet af variable, herunder ses at summen er 6, da der er 6 variable i datasættet. Vi ser hvor stor en del af variationen hver faktor forklarer, når der er 6 faktorer. Faktor 5 og faktor 6 bibringer meget lidt yderligere forklaring af variationen. ev &lt;- eigen(cor(data)) ev$values ## [1] 2.45701130 1.68900056 0.89157047 0.60583326 0.27285334 0.08373107 sum(ev$values) ## [1] 6 ev$values/sum(ev$values) ## [1] 0.40950188 0.28150009 0.14859508 0.10097221 0.04547556 0.01395518 Når vi benytter Eigenvalues metoden, bestemmes antallet af faktorer ud fra antallet af Eigenvalues større end 1, det er denne metode fx. SPSS benytter. Vi ser at kun 2 Eigenvalues er 2.45701130 og 1.68900056 er større end 1, vi får således kun 2 faktorer. Eigenvalues metoden er ofte lidt konservativ, således at vi får færre faktorer end med de øvrige metoder. 5.4.2 Screeplot metoden Vi kan se på nedenstående screeplot, der hvor kurven flader ud, bibringer yderligere faktorer ikke synderlig megen yderligere forklaring til modellen. Man vil med screeplot kriteriet, vælge antallet af faktorer hvor kurven knækkker i tilfældet med 3 faktorer. Man vil ikke altid entydigt kunne afgøre hvor screeplot kurven knækker, her må man så afgøre dette bedst muligt. screeplot(princomp(data),type=&quot;line&quot;,npcs = 6, main=&quot;Screeplot Computereksempel&quot;) Spørgsmål personality Stanford Hent nu filen personality, hvor 240 Stanford studerende har svaret på i hvor høj grad de mener at besidde 32 forskellige personlighedstræk. 1 til 8 hvor 1 er mindst og 8 er mest. Foretag hvis en test viser dette er fordelagtigt en faktoranalyse. Download personality fra filen her og importer den i R via File - Import Dataset. Svar personality Stanford .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } pacman::p_load(psych) cortest.bartlett(personality) ## R was not square, finding R from data ## $chisq ## [1] 4009.545 ## ## $p.value ## [1] 0 ## ## $df ## [1] 496 KMO(personality) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = personality) ## Overall MSA = 0.84 ## MSA for each item = ## distant talkatv carelss hardwrk anxious agreebl tense kind opposng ## 0.88 0.86 0.82 0.87 0.82 0.73 0.84 0.81 0.79 ## relaxed disorgn outgoin approvn shy discipl harsh persevr friendl ## 0.86 0.75 0.87 0.89 0.87 0.84 0.85 0.86 0.87 ## worryin respnsi contrar sociabl lazy coopera quiet organiz criticl ## 0.81 0.86 0.83 0.90 0.89 0.83 0.87 0.78 0.87 ## lax laidbck withdrw givinup easygon ## 0.81 0.73 0.90 0.89 0.78 Vi gennemfører klart analysen viser begge tests. #cor(personality) korrelationsmatricen er udeladt af pladshensyn. corrplot(cor(personality), order = &quot;hclust&quot;, tl.col=&#39;black&#39;, tl.cex=.5) screeplot(princomp(personality),type=&quot;line&quot;, main=&quot;Screeplot personality&quot;) evp &lt;- eigen(cor(personality)) evp$values ## [1] 7.2407068 4.5250901 3.1240573 2.3335890 1.8783611 1.1940636 0.9268636 ## [8] 0.8553802 0.7968460 0.7128793 0.6936229 0.6396893 0.6277046 0.5399600 ## [15] 0.5074253 0.4719721 0.4639916 0.4455816 0.4376430 0.4135706 0.3906941 ## [22] 0.3696839 0.3273829 0.3014077 0.2940833 0.2800131 0.2522101 0.2349649 ## [29] 0.2199461 0.2011492 0.1658493 0.1336176 Vi benytter her 7 faktorer: fapers7 &lt;- factanal(personality,7) fapers7 ## ## Call: ## factanal(x = personality, factors = 7) ## ## Uniquenesses: ## distant talkatv carelss hardwrk anxious agreebl tense kind opposng ## 0.529 0.377 0.491 0.416 0.347 0.576 0.273 0.416 0.512 ## relaxed disorgn outgoin approvn shy discipl harsh persevr friendl ## 0.390 0.181 0.251 0.640 0.387 0.478 0.517 0.560 0.377 ## worryin respnsi contrar sociabl lazy coopera quiet organiz criticl ## 0.388 0.411 0.392 0.367 0.421 0.528 0.284 0.229 0.562 ## lax laidbck withdrw givinup easygon ## 0.663 0.248 0.362 0.557 0.549 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7 ## distant 0.586 0.121 -0.172 0.246 0.137 ## talkatv -0.760 0.144 0.105 ## carelss -0.280 0.137 -0.134 0.192 0.586 0.111 ## hardwrk -0.182 0.700 0.141 -0.187 ## anxious 0.153 0.774 0.136 ## agreebl 0.577 -0.194 0.212 ## tense 0.150 0.787 0.208 -0.179 ## kind -0.102 0.188 0.699 -0.118 -0.189 ## opposng -0.115 0.672 ## relaxed -0.104 -0.526 0.263 0.491 ## disorgn -0.340 0.830 ## outgoin -0.834 0.107 0.166 0.113 ## approvn -0.284 0.137 0.438 -0.142 0.214 ## shy 0.724 -0.240 0.153 ## discipl 0.698 -0.164 ## harsh -0.265 0.619 0.132 ## persevr -0.134 0.598 0.216 ## friendl -0.504 0.129 0.571 -0.131 ## worryin 0.163 0.722 0.119 -0.216 ## respnsi 0.578 0.331 -0.343 -0.152 ## contrar 0.159 -0.162 0.730 0.120 ## sociabl -0.733 0.263 ## lazy 0.167 -0.664 0.152 0.163 0.208 0.130 ## coopera 0.136 -0.123 0.603 -0.248 ## quiet 0.789 -0.157 0.191 0.173 ## organiz 0.433 -0.753 ## criticl 0.112 0.124 0.614 -0.122 ## lax -0.355 0.102 0.227 0.377 ## laidbck -0.102 -0.288 0.109 0.801 ## withdrw 0.720 0.172 -0.150 0.224 0.105 ## givinup 0.337 -0.438 0.280 -0.165 0.136 0.115 ## easygon -0.159 -0.116 -0.245 0.302 0.508 ## ## Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7 ## SS loadings 4.501 3.076 2.494 2.403 2.221 2.068 1.560 ## Proportion Var 0.141 0.096 0.078 0.075 0.069 0.065 0.049 ## Cumulative Var 0.141 0.237 0.315 0.390 0.459 0.524 0.573 ## ## Test of the hypothesis that 7 factors are sufficient. ## The chi square statistic is 460.58 on 293 degrees of freedom. ## The p-value is 0.00000000137 Vi benytter her 8 faktorer: fapers8 &lt;- factanal(personality,8) fapers8 ## ## Call: ## factanal(x = personality, factors = 8) ## ## Uniquenesses: ## distant talkatv carelss hardwrk anxious agreebl tense kind opposng ## 0.534 0.375 0.484 0.425 0.342 0.204 0.263 0.439 0.533 ## relaxed disorgn outgoin approvn shy discipl harsh persevr friendl ## 0.417 0.170 0.257 0.649 0.364 0.490 0.468 0.497 0.333 ## worryin respnsi contrar sociabl lazy coopera quiet organiz criticl ## 0.347 0.420 0.391 0.365 0.420 0.502 0.276 0.236 0.565 ## lax laidbck withdrw givinup easygon ## 0.682 0.005 0.367 0.550 0.580 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7 Factor8 ## distant 0.587 0.103 -0.118 0.262 0.134 ## talkatv -0.761 0.152 0.104 ## carelss -0.307 0.115 0.218 0.584 -0.108 ## hardwrk -0.182 0.694 0.139 -0.188 ## anxious 0.149 0.772 0.144 ## agreebl 0.795 -0.173 0.140 -0.327 ## tense 0.150 0.796 0.211 -0.152 ## kind -0.118 0.227 0.615 -0.151 -0.182 0.238 ## opposng 0.102 -0.116 0.654 ## relaxed -0.108 -0.535 0.276 0.441 ## disorgn -0.336 0.836 ## outgoin -0.836 0.107 0.147 ## approvn -0.294 0.129 0.445 -0.135 0.163 ## shy 0.729 -0.224 0.161 0.153 ## discipl 0.677 -0.168 -0.105 ## harsh -0.178 0.658 0.124 -0.195 ## persevr -0.133 0.644 0.127 0.206 ## friendl -0.517 0.170 0.471 -0.160 0.326 ## worryin 0.164 0.744 0.101 -0.170 0.177 ## respnsi 0.599 0.285 -0.337 -0.117 0.105 ## contrar 0.153 -0.152 0.732 0.118 ## sociabl -0.737 0.222 0.104 0.123 ## lazy 0.166 -0.665 0.142 0.166 0.208 0.116 ## coopera -0.112 0.143 -0.130 0.614 -0.252 ## quiet 0.789 -0.147 0.195 0.169 0.109 ## organiz 0.434 -0.748 ## criticl 0.124 0.132 0.604 -0.119 ## lax -0.361 0.107 0.231 0.323 ## laidbck -0.277 0.105 0.942 ## withdrw 0.721 0.152 -0.111 0.241 0.101 ## givinup 0.337 -0.465 0.254 -0.106 0.161 0.108 ## easygon -0.169 -0.117 -0.261 0.284 0.467 ## ## Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7 ## SS loadings 4.549 3.181 2.531 2.325 2.287 2.052 1.583 ## Proportion Var 0.142 0.099 0.079 0.073 0.071 0.064 0.049 ## Cumulative Var 0.142 0.242 0.321 0.393 0.465 0.529 0.578 ## Factor8 ## SS loadings 0.539 ## Proportion Var 0.017 ## Cumulative Var 0.595 ## ## Test of the hypothesis that 8 factors are sufficient. ## The chi square statistic is 398.53 on 268 degrees of freedom. ## The p-value is 0.000000379 Spørgsmål Mediedata Download filen om mediedata her. Importer denne i R, husk under importen at vælge det korrekte sheet, der indeholder data. Datasættet omhandler danskernes medievaner, 324 danskere er blevet spurgt om deres medievaner. Datasættet indeholder følgende 11 variable. TV-kigning samlet Antal minutter pr. dag. Radiolytning Samlet Antal minutter pr. dag. Avislæsning Samlet Antal minutter pr. dag. TV-kigning Nyheder Antal minutter pr. dag. “Nyheder” omfatter “Nyheder, politik og aktuelt” Radiolytning Nyheder Antal minutter pr. dag. “Nyheder” omfatter “Nyheder, politik og aktuelt” Avislæsning Nyheder Antal minutter pr. dag. “Nyheder” omfatter “Nyheder, politik og aktuelt” Internetforbrug 0 Ingen internetadgang hjemme eller på arbejde 1 Bruger aldrig 2 Mindre end en gang om måneden 3 En gang om måneden 4 Flere gange om måneden 5 En gang om ugen 6 Flere gange om ugen 7 Hver dag Alder Alder i år Højest fuldførte uddannelse 1 Folkeskole 6.-8. klasse 2 Folkeskole 9.-10. klasse 3 Gymnasielle uddannelser, studentereksamen, HF, HHX, HTX 4 Kort erhvervsudd. under 1-2 års varighed, F.eks AMU Arbejdsmarkedsudd., Basisår Erhvervsfaglige udd. 5 Faglig udd. (håndværk, handel, landbrug mv.), F.eks. Faglærte, Social- og sundhedsassistent-udd. og tilsvarende 6 Kort videreg. udd af op til 2-3 år, F.eks. Erhv.akademi, datamatiker, tandplejer, byggetekniker, installatør, HD 7 Mellemlang videreg.udd. 3-4 år. Prof.bachelorer, F.eks. Diploming, sygeplejerske, skolelærer, pædagog, journalist, HA 8 Universitetsbachelor. 1. del af kandidatuddannelse 9 Lang videregående uddannelse. Kandidatuddannelser af 5.-6. års varighed, F.eks. Cand.mag., cand.jur., cand.polyt. etc. 10 Forskeruddannelse. Ph.d., doktor Kvindedummy Dummy variabel kodet med kvinde=1, mand=0 Hjemmeboendebørndummy Dummy variabel kodet med Ja=1, Nej=0 Gennemfør en faktor analyse på datasættet. Spørgsmål Genderroles Download filen genderroles her og importer den i R via File - Import Dataset. Pas på variablene skal konverteres til dummy variable, hvor det giver mening. Man bør nok udelade variablen Region, ellers skal den ændres til et passende antal dummy variable. Gennemfør en faktor analyse på datasættet. Spørgsmål Valgfri datasæt Find et valgfrit datasæt fx. på nettet, gennemfør en faktoranalyse på dette datasæt. "],
["klyngeanalyse.html", "Kapitel 6 Klyngeanalyse 6.1 Hierakisk klyngeanalyse hclust kommandoen 6.2 Ikke hierakisk klyngeanalyse kmeans kommandoen 6.3 Validering med ANOVA", " Kapitel 6 Klyngeanalyse .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Vi har et datasæt bestående af 32 bilmodeller med 11 variable: mpg Miles/(US) gallon cyl Number of cylinders disp Displacement (cu.in.) hp Gross horsepower drat Rear axle ratio wt Weight (1000 lbs) qsec 1/4 mile time vs Engine (0 = V-shaped, 1 = straight) am Transmission (0 = automatic, 1 = manual) gear Number of forward gears carb Number of carburetors Vi vil gerne gruppere de forskellige bilmodeller i forskellige grupper eller klynger udfra deres specifikationer. For at undersøge, om vi på baggrund af tekniske karakteristika, kan gruppere bilmodellerne, benytter vi klyngeanalyse. Bemærk i faktoranalysen grupperer vi variablene, i klyngeanalysen grupperer vi respondenterne eller observationerne, her altså bilerne. Der findes overordnet 2 typer af klyngeudvælgelse: Ikke-hierarkisk, k-means metoden benyttes, hvis vi har store datasæt hvor der kræves mange observationer, man vælger på forhånd hvor mange klynger man vil have. Hierarkisk klyngedannelse, agglomerative metode hvor man starter med at hver respondent har sin egen klynge og man derefter sammenhober disse trin for trin kaldes den sammenhobede eller agglomerative metode. Vi benytter til bildatasættet den agglomerative metode, da vi ikke har en stort datasæt, er denne klart at foretrække. pacman::p_load(&quot;datasets&quot;)#Vi henter pakken datasets der indeholder en del datasæt head(mtcars) #Vi kan se starten af datasættet med Head kommandoen ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Vi kan se at når vi sammenligner forskellige variable ser det ud til at der er forskellige grupper. Nedenfor ser vi fx de 32 biler plottet i et diagram efter hestekræfter og miles per gallon. Bilerne er farvekodet med antal cylindre. #Her angiver vi modelnavne for hver bil #med label rownames vi bruger geom_text i stedet for point her. pacman::p_load(&quot;ggplot2&quot;) ggplot(mtcars, aes(hp, mpg, color = cyl)) + geom_point() #Plot med kun punkter ggplot(mtcars, aes(hp, mpg, color = cyl,label=rownames(mtcars)))+ geom_text(size=3,check_overlap = TRUE) Vi kan benytte t til at transponere data matricen, så kan vi tegne et corrplot, det er meget mørkt da alle bilerne er positivt korrelerede, men man kan ane nogle sammenhænge. t(mtcars) betyder vi transponerer (vender) matricen, så ser vi i stedet på grupper af respondenter, som vi netop analyserer i klyngeanalysen. Hvis ikke vi vender matricen ser vi på korrelationsmatricen mellem de 11 variable i stedet, ligesom vi tidligere har gjort med faktoranalysen. #korrelationsmatricen for transponeret mtcars data, hclust betyder vi ordner efter variable der passer sammen pacman::p_load(corrplot) corrplot(cor(t(mtcars)), order = &quot;hclust&quot;, tl.col=&#39;black&#39;, tl.cex=.5) #Her er korrelationsmatricen ikke transponeret, hvilket svarer til en matrice baseret på variable som ved faktoranalysen vi tidligere så på. corrplot(cor(mtcars), order = &quot;hclust&quot;, tl.col=&#39;black&#39;, tl.cex=.5) 6.1 Hierakisk klyngeanalyse hclust kommandoen .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Vi får nu R til at danne klynger vha. af hclust (hierarcical cluster) kommandoen, denne benytter default en metode der hedder complete til at finde ens klynger, der findes mange andre metoder. For at benytte hclust skal R først beregne afstandene mellem bilerne dette gøres med dist kommandoen. Algoritmen beregner afstandene mellem de forskellige bilmodeller vha. af den euklidiske metrik. Biler med kort afstand kommer i klynger sammen, biler med lang afstand kommer i forskellige klynger. Nedenfor ses et udsnit af afstandende mellem hver af de 32 biler, det er en meget stor 32 \\(\\times\\) 32 matrice, derfor har vi benyttet head for kun at vise noget af matricen. Fx er afstanden mellem to forskellige biler som en Mazda RX4 og en Lincoln Continental 318.05 hvilket er en stor afstand i forhold til fx. Mazda RX4 og Mazda RX4 Wag på kun 0.62. Bemærk hvordan supersportsvognen Maserati Bora har store afstande til de fleste af de øvrige biler, Maseratien var en komfortabel, rummeligere og kraftigere og tungere sportsvogn end fx. Ferrari Dino. head(as.matrix(dist(mtcars))) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## Mazda RX4 0.0000000 0.6153251 54.90861 98.11252 ## Mazda RX4 Wag 0.6153251 0.0000000 54.89152 98.09589 ## Datsun 710 54.9086059 54.8915169 0.00000 150.99352 ## Hornet 4 Drive 98.1125212 98.0958939 150.99352 0.00000 ## Hornet Sportabout 210.3374396 210.3358546 265.08316 121.02976 ## Valiant 65.4717710 65.4392224 117.75470 33.55087 ## Hornet Sportabout Valiant Duster 360 Merc 240D ## Mazda RX4 210.3374 65.47177 241.40765 50.15327 ## Mazda RX4 Wag 210.3359 65.43922 241.40887 50.11461 ## Datsun 710 265.0832 117.75470 294.47902 49.65848 ## Hornet 4 Drive 121.0298 33.55087 169.42996 121.27397 ## Hornet Sportabout 0.0000 152.12414 70.17673 241.50697 ## Valiant 152.1241 0.00000 194.60945 89.59111 ## Merc 230 Merc 280 Merc 280C Merc 450SE Merc 450SL ## Mazda RX4 25.46831 15.36419 15.67247 135.43070 135.40144 ## Mazda RX4 Wag 25.32845 15.29569 15.58377 135.42548 135.39604 ## Datsun 710 33.18038 66.93635 67.02614 189.19549 189.16317 ## Hornet 4 Drive 118.24331 91.42240 91.46129 72.49643 72.43135 ## Hornet Sportabout 233.49240 199.33450 199.34066 84.38885 84.36840 ## Valiant 85.00796 60.29098 60.26557 90.69703 90.67697 ## Merc 450SLC Cadillac Fleetwood Lincoln Continental ## Mazda RX4 135.47947 326.3396 318.0470 ## Mazda RX4 Wag 135.47232 326.3355 318.0429 ## Datsun 710 189.23454 381.0926 372.8012 ## Hornet 4 Drive 72.57185 234.4404 227.9726 ## Hornet Sportabout 84.43324 116.2804 108.0624 ## Valiant 90.70930 266.6281 259.6304 ## Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla ## Mazda RX4 304.72034 93.26800 102.83076 100.6040 ## Mazda RX4 Wag 304.71692 93.25310 102.82387 100.5888 ## Datsun 710 359.30149 40.99338 52.77046 47.6535 ## Hornet 4 Drive 218.15483 184.96897 191.55187 192.6714 ## Hornet Sportabout 97.20491 302.03772 310.03246 309.5582 ## Valiant 248.77133 152.11533 158.96158 159.8303 ## Toyota Corona Dodge Challenger AMC Javelin Camaro Z28 ## Mazda RX4 42.30752 163.11508 149.60472 233.22288 ## Mazda RX4 Wag 42.26592 163.11342 149.60145 233.22487 ## Datsun 710 12.96547 217.77958 204.31889 286.00492 ## Hornet 4 Drive 138.53047 72.44039 61.36019 163.66326 ## Hornet Sportabout 252.33320 48.98389 61.42742 70.96653 ## Valiant 105.28764 103.43107 91.04443 187.84638 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa ## Mazda RX4 248.67803 92.50484 44.40337 65.73284 ## Mazda RX4 Wag 248.67620 92.49400 44.40736 65.73626 ## Datsun 710 303.35839 39.88151 13.13571 25.09486 ## Hornet 4 Drive 156.22403 184.44712 139.15795 163.23674 ## Hornet Sportabout 40.00525 301.56695 254.14526 272.35824 ## Valiant 188.52721 151.43794 106.05858 130.82482 ## Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E ## Mazda RX4 245.4247 66.76610 265.6454 39.18940 ## Mazda RX4 Wag 245.4294 66.77642 265.6491 39.16260 ## Datsun 710 297.2940 90.24155 309.7718 20.69394 ## Hornet 4 Drive 180.1140 130.55230 229.3419 137.03633 ## Hornet Sportabout 89.5934 215.06739 170.7094 248.00634 ## Valiant 203.0178 106.56948 242.4393 104.18637 Vi gemmer hclust data i clusters variablen, vi så kan benytte til at tegne en oversigt over klyngerne. Vi kan nu plotte en grafisk oversigt over bilerne. I nederste linje er den fineste inddeling, hvor samtlige biler er i deres egen klynge. Den blå linje med 3 skæringer i dendogrammet indikerer der er 3 klynger, den røde 4 klynger. clusters &lt;- hclust(dist(mtcars)) plot(clusters,cex=0.5,main = &quot;Dendogram af mtcars&quot;,xlab = &quot;Klyngetræ&quot;,sub=&quot;Bilmodeller&quot;) abline(h = 190, col=&quot;red&quot;) #Tegn rød vandret linje h betyder horisontal abline(h = 230, col=&quot;blue&quot;) Hvis vi ønsker at undersøge en indeling med et bestemt antal klynger, kan vi bruge cutree i R, til at undersøge klyngerne i en skæring med fx. 4 klynger nærmere. Her ser vi som nævnt, Maserati Bora skiller sig ud ved at have sin egen klynge. Nummeret ved hver af de 32 biler angiver hvilken klynge bilen tilhører. clusterCut &lt;- cutree(clusters, 4) #Opdeling i 4 klynger. clusterCut ## Mazda RX4 Mazda RX4 Wag Datsun 710 ## 1 1 1 ## Hornet 4 Drive Hornet Sportabout Valiant ## 2 3 2 ## Duster 360 Merc 240D Merc 230 ## 3 1 1 ## Merc 280 Merc 280C Merc 450SE ## 1 1 2 ## Merc 450SL Merc 450SLC Cadillac Fleetwood ## 2 2 3 ## Lincoln Continental Chrysler Imperial Fiat 128 ## 3 3 1 ## Honda Civic Toyota Corolla Toyota Corona ## 1 1 1 ## Dodge Challenger AMC Javelin Camaro Z28 ## 2 2 3 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 ## 3 1 1 ## Lotus Europa Ford Pantera L Ferrari Dino ## 1 3 1 ## Maserati Bora Volvo 142E ## 4 1 Vi kan benytte subset kommandoen til at se på hvilke variable der er i hver klynge, herunder ser vi på klynge 3. subset(clusterCut,clusterCut==3) ## Hornet Sportabout Duster 360 Cadillac Fleetwood ## 3 3 3 ## Lincoln Continental Chrysler Imperial Camaro Z28 ## 3 3 3 ## Pontiac Firebird Ford Pantera L ## 3 3 Vi kan ligeledes sammenligne klyngeinddelingen med de enkelte variable og se om disse passer sammen. Fx. passer hp meget fint med inddelingen i klynger. table(clusterCut, mtcars$cyl) ## ## clusterCut 4 6 8 ## 1 11 5 0 ## 2 0 2 5 ## 3 0 0 8 ## 4 0 0 1 table(clusterCut, mtcars$mpg) ## ## clusterCut 10.4 13.3 14.3 14.7 15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 ## 1 0 0 0 0 0 0 0 0 0 0 1 0 0 ## 2 0 0 0 0 0 2 1 0 1 1 0 1 0 ## 3 2 1 1 1 0 0 0 1 0 0 0 0 1 ## 4 0 0 0 0 1 0 0 0 0 0 0 0 0 ## ## clusterCut 19.2 19.7 21 21.4 21.5 22.8 24.4 26 27.3 30.4 32.4 33.9 ## 1 1 1 2 1 1 2 1 1 1 2 1 1 ## 2 0 0 0 1 0 0 0 0 0 0 0 0 ## 3 1 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 0 0 table(clusterCut, mtcars$hp) ## ## clusterCut 52 62 65 66 91 93 95 97 105 109 110 113 123 150 175 180 205 215 ## 1 1 1 1 2 1 1 1 1 0 1 2 1 2 0 1 0 0 0 ## 2 0 0 0 0 0 0 0 0 1 0 1 0 0 2 0 3 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 1 ## 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## ## clusterCut 230 245 264 335 ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 1 2 1 0 ## 4 0 0 0 1 table(clusterCut, mtcars$carb) ## ## clusterCut 1 2 3 4 6 8 ## 1 5 6 0 4 1 0 ## 2 2 2 3 0 0 0 ## 3 0 2 0 6 0 0 ## 4 0 0 0 0 0 1 table(clusterCut, mtcars$wt) ## ## clusterCut 1.513 1.615 1.835 1.935 2.14 2.2 2.32 2.465 2.62 2.77 2.78 ## 1 1 1 1 1 1 1 1 1 1 1 1 ## 2 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 0 ## ## clusterCut 2.875 3.15 3.17 3.19 3.215 3.435 3.44 3.46 3.52 3.57 3.73 3.78 ## 1 1 1 0 1 0 0 2 0 0 0 0 0 ## 2 0 0 0 0 1 1 0 1 1 0 1 1 ## 3 0 0 1 0 0 0 1 0 0 1 0 0 ## 4 0 0 0 0 0 0 0 0 0 1 0 0 ## ## clusterCut 3.84 3.845 4.07 5.25 5.345 5.424 ## 1 0 0 0 0 0 0 ## 2 0 0 1 0 0 0 ## 3 1 1 0 1 1 1 ## 4 0 0 0 0 0 0 Hvis dendogrammet virker lidt uoverskueligt, kan man vælge ape pakken for at lave mere fancy plots, her er rigtig mange muligheder. #install.packages(&quot;ape&quot;) library(&quot;ape&quot;) colors = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;pink&quot;) clus4 = cutree(clusters, 4) plot(as.phylo(clusters), type = &quot;fan&quot;, tip.color = colors[clus4], label.offset = 0, cex = 0.5) Herunder er et plot, hvor farvekoden er baseret på klyngerne. ggplot(mtcars, aes(hp, mpg)) + geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) 6.2 Ikke hierakisk klyngeanalyse kmeans kommandoen .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Vi kunne også have brugt kmeans metoden, her skal vi så angive hvor mange klynger, vi ønsker i analysen. Her benytter vi K-means og får 4 klynger med 7, 6, 9, 10 biler. I output fra R kan vi under Cluster means se gennemsnit, for de 4 klynger for alle 11 variable. mtcarsCluster4 &lt;- kmeans(mtcars, 4) mtcarsCluster4 ## K-means clustering with 4 clusters of sizes 9, 10, 6, 7 ## ## Cluster means: ## mpg cyl disp hp drat wt qsec vs ## 1 14.64444 8.000000 388.2222 232.1111 3.343333 4.161556 16.40444 0.0000000 ## 2 27.05000 4.000000 101.5700 81.4000 4.086000 2.199300 18.76100 0.9000000 ## 3 16.83333 7.666667 284.5667 158.3333 3.033333 3.625000 17.76833 0.1666667 ## 4 19.94286 5.714286 166.5714 120.1429 3.705714 3.107857 18.47143 0.5714286 ## am gear carb ## 1 0.2222222 3.444444 4.000000 ## 2 0.8000000 4.100000 1.500000 ## 3 0.0000000 3.000000 2.333333 ## 4 0.4285714 4.000000 3.571429 ## ## Clustering vector: ## Mazda RX4 Mazda RX4 Wag Datsun 710 ## 4 4 2 ## Hornet 4 Drive Hornet Sportabout Valiant ## 3 1 4 ## Duster 360 Merc 240D Merc 230 ## 1 2 4 ## Merc 280 Merc 280C Merc 450SE ## 4 4 3 ## Merc 450SL Merc 450SLC Cadillac Fleetwood ## 3 3 1 ## Lincoln Continental Chrysler Imperial Fiat 128 ## 1 1 2 ## Honda Civic Toyota Corolla Toyota Corona ## 2 2 2 ## Dodge Challenger AMC Javelin Camaro Z28 ## 3 3 1 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 ## 1 2 2 ## Lotus Europa Ford Pantera L Ferrari Dino ## 2 1 4 ## Maserati Bora Volvo 142E ## 1 2 ## ## Within cluster sum of squares by cluster: ## [1] 46659.317 10247.471 6355.581 8808.032 ## (between_SS / total_SS = 88.4 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; Nedenfor ser vi på en tabel inddeling med 4 klynger med de 32 biler, sorteret efter antallet af cylindre. table(mtcarsCluster4$cluster, mtcars$cyl) ## ## 4 6 8 ## 1 0 0 9 ## 2 10 0 0 ## 3 0 1 5 ## 4 1 6 0 Nedenfor ser vi på en tabel inddeling med 3 klynger med de 32 biler, sorteret efter antallet af cylindre. Kører vi kmeans analysen igen, falder bilerne ikke nødvendigvis i samme klynger som tidligere, det skyldes algoritmen kan give forskellig optimale inddelinger. mtcarsCluster3 &lt;- kmeans(mtcars, 3) table(mtcarsCluster3$cluster, mtcars$cyl) ## ## 4 6 8 ## 1 11 0 0 ## 2 0 0 14 ## 3 0 7 0 Vi kan lave et klyngeplot der viser forskellene på de 32 biler, herunder ses plottet med 3 klynger. Bemærk vi skal hente pakken factoextra, der indeholder plotfunktionen fviz_cluster(). Vi har benyttet funktionen scale(), det er en rigtig god ide at benytte hvis, der er stor forskel på måleenhederne i en data.frame. Funktionen scale() bringer variablene i samme skale. Der er fx. stor forskel på enhederne i carb og disp, prøv at sammenligne nedenstående plot med et tilsvarende plot uden scale. km3.res &lt;- kmeans(scale(mtcars), 3, nstart = 25) pacman::p_load(factoextra) fviz_cluster(km3.res, data = mtcars, main = &quot;Klyngeplot biler opdelt i 3 klynger&quot;,repel = TRUE) Vi kan lave et klyngeplot der viser forskellene på de 32 biler, herunder ses plottet med 4 klynger. km4.res &lt;- kmeans(scale(mtcars), 4, nstart = 25) fviz_cluster(km4.res, data = mtcars, main = &quot;Klyngeplot biler opdelt i 4 klynger&quot;,repel = TRUE) Vi kan lave et klyngeplot der viser forskellene på de 32 biler, herunder ses plottet med 5 klynger. km5.res &lt;- kmeans(scale(mtcars), 5, nstart = 25) fviz_cluster(km5.res, data = mtcars, main = &quot;Klyngeplot biler opdelt i 5 klynger&quot;,repel = TRUE) 6.3 Validering med ANOVA Skal man undersøge om grupperne/klyngerne er forskellige med hensyn til de forskellige variable, kan man benytte Anova, hvor klyngerne er den uafhængige variabel. Man skal da gerne nå frem til at klyngegennemsnittede er signifikant forskellige mht. flere af variablene der indgår i analysen. Spørgsmål US arrestationer samt urbaniseringsgrad Lav en klyngeanalyse for datasæt med 50 observationer for amerikanske stater på 4 variable, data stammer fra World Almanac and Book of facts 1975. (Crime rates). Mord, antal arrestationer (pr 100,000) Overfald, antal arrestationer (pr 100,000) Urbaniseringsgrad andel af bybefolkning. Voldtægt, antal arrestationer (pr 100,000) pacman::p_load(datasets) arrest &lt;- USArrests head(arrest) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 Undersøg om der kan dannes klynger og hvorledes disse kan karakteriseres. Illustrer grafisk og kommenter på karakteristika for klyngerne. Svar US arrestationer samt urbaniseringsgrad kort version .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Spørgsmål GDP Download filen om GDP og GDP per capita her. Importer denne i R. Sørg for at få navnene for de enkelte lande som label, dette kan du fx. gøre ved nedenstående kommandoer: Hvad betyder: GDP[1:50,] og GDP[,1] GDP &lt;- GDP[1:50,] row.names(GDP) &lt;- as.matrix(GDP[,1]) plot(clusters,cex=0.5,main = &quot;GDP&quot;,xlab = &quot;Klyngetræ&quot;,sub=&quot;GDP&quot;) Undersøg om der kan dannes klynger og hvorledes disse kan karakteriseres, der er rigtig mange observationer dvs. lande, se i stedet på deldatasæt der giver mening. Illustrer grafisk og kommenter på karakteristika for klyngerne. Spørgsmål Forbes 100 US Download filen om de 100 rigeste i USA her. Filen er tilrettet, dvs. binære kvalitative variable er kodet om til dummy variable. Undersøg om der kan dannes klynger og hvorledes disse kan karakteriseres. Illustrer grafisk og kommenter på karakteristika for klyngerne. Bemærk det er godt at sætte navnene på de velhavende som rækkenavne i din data.frame, for at det er nemmere at få et overblik i klyngetræ diagrammet, dette kan du fx. gøre som nedenfor: row.names(Forbes100) &lt;- as.matrix(Forbes100[,1]) clusters &lt;- hclust(dist(Forbes100)) plot(clusters,cex=0.5,main = &quot;US 100 Rigeste&quot;,xlab = &quot;Klyngetræ&quot;,sub=&quot;US top 100&quot;) Spørgsmål Valgfri datasæt Find et valgfrit datasæt fx. på nettet, gennemfør en klyngeanalyse på dette datasæt. "],
["tidsrkker-og-arima.html", "Kapitel 7 Tidsrækker og ARIMA 7.1 ARIMA(0,0,0) 7.2 ARIMA(1,0,0) eller AR(1) autoregression 7.3 ARIMA(0,1,0) eller I(1) Random Walk with a drift 7.4 ARIMA(0,0,1) eller MA(1) Moving average 7.5 Plots med forskellige modeller 7.6 ARIMA af højere orden 7.7 ARIMA og sæsonalitet 7.8 ARIMA eksempler 7.9 Forecast Aktiekurser 7.10 Aktieafkast", " Kapitel 7 Tidsrækker og ARIMA Gennemgangen af tidsrækkeanalyse bygger meget på praktisk anvendelighed (dvs. vi vil gerne kunne forudsige kursudviklingen), vi vil springe let hen over teorien der kan være tung og er meget omfattende. Nedenstående links giver dog en indføring i den teoretiske del, som vi her ikke berører. http://ucanalytics.com/blogs/arima-models-manufacturing-case-study-example-part-3/ http://ucanalytics.com/blogs/step-by-step-graphic-guide-to-forecasting-through-arima-modeling-in-r-manufacturing-case-study-example/ Her er en gennemgang af forskellige typer af tidsrækker man kan opleve. https://people.duke.edu/~rnau/411arim.htm#arima010 Video om ARIMA https://youtu.be/Aw77aMLj9uM En tidsrække er observationer, der er observeret over tid, fx. lukkekursen på Novo i 2018, kan vi beskrive som en tidsrække. Hvor vi både registrerer dato og lukkekursen. ARIMA er et avanceret analyseværktøj til at beskrive tidsrækker. Vi vil i de følgende kaptiler, med eksempler beskrive hvordan de enkelte elementer i ARIMA rent praktisk fungerer. AR står for AutoRegressive I står for Integrated MA står for Moving Average Lad i de følgende afsnit se på nogle simple eksempler for trinvis, at kunne beskrive hvorledes modellen fungerer. 7.1 ARIMA(0,0,0) .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Nedenfor har vi aktiekurser for 50 dage for en fiktiv aktie, vi vil nu undersøge om disse kan bruges til at forudsige noget om fremtidige aktiekurser. For at gøre dette, skal man enten importere Excelfilen, eller copy paste data fra rammen nedenfor. Hent ARIMA1.xlsx Excel filen her. Importer ARIMA1.xlsx til R via menuen File - Import Dataset - Excel. Nu skal datasættet rettes til en tidsserie med ts() kommandoen. ARIMA1 &lt;- ts(ARIMA1) Vi kan nu plotte vore data i R. plot.ts(ARIMA1, xlab=&#39;Tid&#39;, ylab = &#39;Kursdata&#39;) Det er svært at se nogen tydelig udvikling i kursen. Vi benytter auto.arima til at undersøge om der er en systematik i tidsserien, for at bruge denne funktion skal vi hente og loade pakken forecast med fx. pacman: Funktionen auto.arima i R er en fantastisk funktion, der automatisk finder den ARIMA model, der passer bedst på observationerne. auto.arima(ARIMA1) ## Series: ARIMA1 ## ARIMA(0,0,0) with non-zero mean ## ## Coefficients: ## mean ## 19.8964 ## s.e. 0.2872 ## ## sigma^2 estimated as 4.209: log likelihood=-106.37 ## AIC=216.74 AICc=217 BIC=220.57 Output ARIMA(0,0,0) with non-zero mean, fortæller os at data er ligesom hvid støj. Den bedste forudsigelse af aktieprisen, vi kan komme med er gennemsnittet af alle kurserne. Vi kan altså ikke forudsige prisen vha. vore fine værktøjer. Akaike Information Criterion (AIC) , og Bayesian Information Criterion (BIC) benyttes til at vælge ARIMA modellen med mindst AIC og BIC værdier. auto.arima finder den bedste model automatisk. Her er ligningen for aktiekursen, den bedste forudsigelse af den fremtidige kurs er den gennemsnitlige kurs der tidligere er observeret. \\[\\hat{Y_t}=19.90\\] Variablen \\(\\hat{Y}_t\\), kaldet Y hat t angiver vort estimat (gæt) på aktiekursen på tidspunkt \\(t=1,2,3,...\\). Der er således så lidt systematik i Data at her er tale om en ARIMA(0,0,0) model. Vi ser også at der står “ARIMA(0,0,0) with non-zero mean” i output fra R. 7.2 ARIMA(1,0,0) eller AR(1) autoregression .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } En ARIMA(1,0,0) model kan skrives som: \\[\\hat{Y_t}=c + \\phi Y_{t-1}\\] Vi kan forklare \\(\\hat{Y_t}\\) er værdien for tidsrækken på tidspunkt \\(t\\), ud fra en konstant \\(c\\) plus en faktor \\(\\phi\\), der ganges på værdien for tidsrækken på tidspunkt \\(t-1\\). For at bestemme c skal vi kende tidsrækkens sande middelværdi \\(\\mu\\) og \\(\\phi\\), disse værdier kan R beregne for os. Vi kan så beregne konstanten \\(c=(1-\\phi)\\cdot \\mu\\). Det betyder så at vi kan estimere fremtidige værdier tidsrækken. Vi har nu et eksempel hvor den sande middelværdi for tidsrækken er \\(\\mu=100\\) og \\(\\phi=0.5\\) for en ARIMA(1,0,0) model. Så kan vi beregne \\(c=(1-\\phi)\\cdot \\mu=(1-0.5)\\cdot 100=50\\) er middelværdien estimeret ved den gennemsnitlige kurs. Ligningen for modellen kan så skrives som: \\[\\hat{Y_t}=c + \\phi Y_{t-1} \\Leftrightarrow \\hat{Y_t}=50 + 0.5 Y_{t-1}\\] \\(\\phi\\) fortæller, hvis kursen dagen før var 80 gennemsnitskursen er 100, vil kursen imorgen \\(t=1\\) ifølge modellen være forudsagt som: \\[50+0.5\\cdot 80=90\\] Dagen efter \\(t=2\\) vil kursen så være forudsagt til: \\[50+0.5\\cdot 90=95\\] Om 3 dage dvs. \\(t=3\\) vil kursen så være forudsagt til: \\[50+0.5\\cdot 95=97.5\\] Om 4 dage dvs. \\(t=4\\) vil kursen så være forudsagt til: \\[50+0.5\\cdot 97.5=98.75\\] Osv.. Vi siger at forudsagte værdier konvergerer mod (dvs. nærmer sig) \\(\\mu=100\\). AR i ARIMA, står for autoregression, selv-regression mod middelværdien, i eksemplet så vi hvordan værdien nærmer sig 100, hvis vi forudsiger flere dages kurser kan vi se dette. \\(\\phi\\) må kun antage værdier mellem og ikke lig med -1 og 1, hvilket betyder den er stationær, altså nærmer sig den sande middelværdi \\(\\mu\\). Hvad vil der ske hvis \\(\\mu=100\\) og \\(\\phi=-0.5\\) for en ARIMA(1,0,0) model (husk \\(c=(1-\\phi)\\cdot\\mu\\) når man skal bestemme modellen)? Hent ARIMA2.xlsx Excel filen her. Importer ARIMA2.xlsx til R via menuen File - Import Dataset - Excel. Nu skal datasættet rettes til en tidsserie med ts() kommandoen. ARIMA2 &lt;- ts(ARIMA2) aaa2 &lt;- auto.arima(ARIMA2) aaa2 ## Series: ARIMA2 ## ARIMA(1,0,0) with non-zero mean ## ## Coefficients: ## ar1 mean ## 0.3664 103.2372 ## s.e. 0.1301 2.4492 ## ## sigma^2 estimated as 128.3: log likelihood=-191.36 ## AIC=388.73 AICc=389.25 BIC=394.46 Her afslører auto.arima 1. ordens autoregression dvs. Modellen kan skrives som. \\[\\hat{Y_t}=c + \\phi Y_{t-1}\\Leftrightarrow \\hat{Y_t}=(1-0.3664)\\cdot 103.2372 + 0.3664Y_{t-1}\\Leftrightarrow \\hat{Y_t}=65.4125 + 0.3664Y_{t-1}\\] Vi ser nu igen på vores eksempel med ARIMA2, vi kan nu i R forudsige aktiekursen 12 perioder frem med predict: predict(auto.arima(ARIMA2), n.ahead = 12)$pred ## Time Series: ## Start = 51 ## End = 62 ## Frequency = 1 ## [1] 105.3673 104.0177 103.5232 103.3420 103.2756 103.2513 103.2424 ## [8] 103.2391 103.2379 103.2375 103.2373 103.2373 Spørgsmål ARIMA(1,0,0) Hent ARIMA22.xlsx Excel filen her. Importer ARIMA22.xlsx til R via menuen File - Import Dataset - Excel. Bestem for den fremtidige aktiekurs 15 perioder frem, udregn direkte fx. vha. Excel og tjek dit resultat i R. Svar ARIMA(1,0,0) ARIMA22 &lt;- ts(ARIMA22) aaa22 &lt;- auto.arima(ARIMA22) aaa22 ## Series: ARIMA22 ## ARIMA(1,0,0) with non-zero mean ## ## Coefficients: ## ar1 mean ## 0.2286 215.4871 ## s.e. 0.1371 2.3978 ## ## sigma^2 estimated as 180.3: log likelihood=-199.82 ## AIC=405.63 AICc=406.15 BIC=411.37 predict(auto.arima(ARIMA22), n.ahead = 15)$pred ## Time Series: ## Start = 51 ## End = 65 ## Frequency = 1 ## [1] 216.7592 215.7779 215.5536 215.5023 215.4905 215.4878 215.4872 ## [8] 215.4871 215.4871 215.4871 215.4871 215.4871 215.4871 215.4871 ## [15] 215.4871 Spørgsmål ARIMA(1,0,0) Hent ARIMA23.xlsx Excel filen her. Importer ARIMA23.xlsx til R via menuen File - Import Dataset - Excel. Bestem for den fremtidige aktiekurs 15 perioder frem, udregn direkte fx. vha. Excel og tjek dit resultat i R. 7.3 ARIMA(0,1,0) eller I(1) Random Walk with a drift .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Hvis en serie er ikke-stationær, er den simpleste model en random walk: \\[\\hat{Y_t}-Y_{t-1}=\\mu\\Leftrightarrow \\hat{Y_t}=Y_{t-1}+\\mu\\] Dette betyder at Y stiger konstant med \\(\\mu\\) i hver periode. Drift betyder at tidsrækken stiger konstant. Forestiller man sig en ARIMA(0,1,0) med drift 10 og en kurs på tidspunkt t-1 på 120, vil vi forudsige en kurs på 130 ved tid t og 140 ved tid t+1 osv. Vi kan opskrive modellen som: \\[\\hat{Y_t}-Y_{t-1}=10\\Leftrightarrow \\hat{Y_t}=Y_{t-1}+10\\] Hent ARIMA3.xlsx Excel filen her. Importer ARIMA3.xlsx til R via menuen File - Import Dataset - Excel. Nu skal datasættet rettes til en tidsserie med ts() kommandoen. ts.plot(ARIMA3) auto.arima(ARIMA3) ## Series: ARIMA3 ## ARIMA(0,1,0) with drift ## ## Coefficients: ## drift ## 7.9149 ## s.e. 1.2790 ## ## sigma^2 estimated as 83.46: log likelihood=-181.05 ## AIC=366.1 AICc=366.36 BIC=369.93 Modellen ovenfor kan skrives som: \\[\\hat{Y_t}-Y_{t-1}=\\mu\\Leftrightarrow \\hat{Y_t}-Y_{t-1}=7.9\\Leftrightarrow \\hat{Y_t}=Y_{t-1}+7.9\\] Vi indsætter drift i stedet for \\(\\mu\\), tolningen er at modellen forudsiger at aktiekursen stiger med 7.9 fra periode til periode. Hvis vi har en ren random walk model uden drift dvs. med \\(\\mu=0\\) ARIMA(0,1,0) for en aktiekurs , forventer vi at kursen til tid t vil være den samme som til tid t-1. Denne kan skrives som: \\[\\hat{Y_t}-Y_{t-1}=0\\] 7.4 ARIMA(0,0,1) eller MA(1) Moving average .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Vi kan i stedet for at bruge tidligere aktiekurser til at forudsige aktiekursen i stedet benytte tidligere målefejl residualer til at forudsige kursen. Modellen kan skrives som: \\[\\hat{Y_t}=\\mu+\\theta_1 e_{t-1}\\] Hvis vi forestiller os \\(\\mu=50\\) \\(\\theta_1=0.5\\) kursen til tid t-1 var 120 forudsigelsen til tid t-1 var 100, så målefejlen residualen til tid t-1 er \\(e_{t-1}\\) er faktisk kurs minus forudsagt kurs altså 120-100=20. Nu kan vi forudsige kursen til tid t som: \\[\\hat{Y_t}=\\mu+\\theta_1 e_{t-1}\\Leftrightarrow \\hat{Y_t}=50+0.5\\cdot20=60\\] Hent ARIMA4.xlsx Excel filen her. Importer ARIMA4.xlsx til R via menuen File - Import Dataset - Excel. Nu skal datasættet rettes til en tidsserie med ts() kommandoen. ts.plot(ARIMA4) auto.arima(ARIMA4) ## Series: ARIMA4 ## ARIMA(0,0,1) with non-zero mean ## ## Coefficients: ## ma1 mean ## 0.9053 99.0176 ## s.e. 0.0664 2.5588 ## ## sigma^2 estimated as 95.68: log likelihood=-184.81 ## AIC=375.62 AICc=376.14 BIC=381.35 Vi kan nu forudsige aktiekursen 12 perioder frem med predict: predict(auto.arima(ARIMA4), n.ahead = 12)$pred ## Time Series: ## Start = 51 ## End = 62 ## Frequency = 1 ## [1] 113.64960 99.01756 99.01756 99.01756 99.01756 99.01756 99.01756 ## [8] 99.01756 99.01756 99.01756 99.01756 99.01756 Hvorfor svarer den forudsagte værdi til mean i en ren ARIMA(0,0,1) eller MA(1) model? (Vink hvad er definitionen på en residual) 7.5 Plots med forskellige modeller ## Series: ap1 ## ARIMA(0,1,1) ## ## Coefficients: ## ma1 ## -0.4683 ## s.e. 0.1334 ## ## sigma^2 estimated as 131: log likelihood=-192.45 ## AIC=388.9 AICc=389.16 BIC=392.73 Kursen svinger omkring middelværdien. ## Series: ap2 ## ARIMA(2,0,1) with non-zero mean ## ## Coefficients: ## ar1 ar2 ma1 mean ## 1.2931 -0.4600 0.5204 127.6227 ## s.e. 0.1729 0.1736 0.1613 10.6416 ## ## sigma^2 estimated as 74.2: log likelihood=-181.86 ## AIC=373.71 AICc=375.04 BIC=383.37 ## Series: ap3 ## ARIMA(1,0,1) with non-zero mean ## ## Coefficients: ## ar1 ma1 mean ## -0.3194 0.7955 100.7927 ## s.e. 0.1092 0.0724 0.9790 ## ## sigma^2 estimated as 105.3: log likelihood=-748.2 ## AIC=1504.4 AICc=1504.61 BIC=1517.6 Vi kan også grafisk vise hvordan kursen vil udvikle sig med 80% og 95% konfidensbælter. forecast(auto.arima(ap3)) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 201 97.76341 84.61387 110.9129 77.65293 117.8739 ## 202 101.76015 87.19609 116.3242 79.48634 124.0340 ## 203 100.48378 85.78310 115.1845 78.00104 122.9665 ## 204 100.89139 86.17685 115.6059 78.38745 123.3953 ## 205 100.76122 86.04527 115.4772 78.25512 123.2673 ## 206 100.80279 86.08670 115.5189 78.29647 123.3091 ## 207 100.78951 86.07341 115.5056 78.28317 123.2959 ## 208 100.79375 86.07764 115.5099 78.28741 123.3001 ## 209 100.79240 86.07629 115.5085 78.28605 123.2987 ## 210 100.79283 86.07672 115.5089 78.28648 123.2992 plot(forecast(auto.arima(ap3))) 7.6 ARIMA af højere orden Arima modeller kan afhænge af flere tidligere perioder, fx kan ligningen for ARIMA(2,0,0) eller AR(2), opskrives som: \\[\\hat{Y_t}=c + \\phi Y_{t-1}+ \\phi_2 Y_{t-2}\\] Modellen afhænger altså af 2 tidligere perioder (lags) og ikke en. Man betegner dette som en model med lag 2. Arima modeller kan indeholde flere forskellige elementer med lag som fx. ARIMA(0,2,1). 7.7 ARIMA og sæsonalitet .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Hvis fx. en aktie handles lavere om fredagen kan ARIMA modellerne korrigere for dette ved sæsonkorrektion. I sæsonkorrigerede modeller vises dette som en ekstra vektor med 3 tal for hhv. sæsonkorrigeret AR eller SAR, sæsonkorrigeret I eller SI og sæsonkorrigeret MA eller SMA. En model som ARIMA(1,0,0)(1,0,0) har altså udover AR også en sæsonkomponent. 7.8 ARIMA eksempler 7.8.1 Traktorer Hent følgende data for traktor salg, med følgende kommandoer i R. data = read.csv(&#39;http://ucanalytics.com/blogs/wp-content/uploads/2015/06/Tractor-Sales.csv&#39;) data = ts(data[,2],start = c(2003,1),frequency = 12) Vi ser salget er voksende over tid, der er ligeledes en sæsonkomponent. plot(data, xlab=&#39;Years&#39;, ylab = &#39;Tractor Sales&#39;) Differens tranformer data for at generere stationære data mht. middel (fjern trend) plot(diff(data),ylab=&#39;Differenced Tractor Sales&#39;) log transformer data for at sikre stationaritet mht. varians. plot(log10(data),ylab=&#39;Log (Tractor Sales)&#39;) Eventuel Differens og log transformation af data for at sikre stationaritet både mht. middel og varians. plot(diff(log10(data)),ylab=&#39;Differenced Log (Tractor Sales)&#39;) Find bedste model med auto.arima, når der er stationaritet. Akaike Information Criterion (AIC) , og Bayesian Information Criterion (BIC), vælg ARIMA modellen med mindst AIC and BIC værdier. auto.arima finder den bedste model automatisk. require(forecast) ARIMAfit = auto.arima(log10(data), approximation=FALSE,trace=FALSE) ARIMAfit ## Series: log10(data) ## ARIMA(0,1,1)(0,1,1)[12] ## ## Coefficients: ## ma1 sma1 ## -0.4047 -0.5529 ## s.e. 0.0885 0.0734 ## ## sigma^2 estimated as 0.0002571: log likelihood=354.4 ## AIC=-702.79 AICc=-702.6 BIC=-694.17 Nu kan vi forudsige kommende traktor salg med modellen par(mfrow = c(1,1)) pred = predict(ARIMAfit, n.ahead = 36) salg &lt;- 10^pred$pred salg ## Jan Feb Mar Apr May Jun Jul ## 2015 567.7645 566.4765 670.8226 758.9138 855.9482 817.2827 938.7239 ## 2016 625.2464 623.8280 738.7384 835.7481 942.6065 900.0265 1033.7626 ## 2017 688.5479 686.9859 813.5300 920.3613 1038.0383 991.1474 1138.4233 ## Aug Sep Oct Nov Dec ## 2015 934.5120 703.5005 626.9879 571.9988 668.5363 ## 2016 1029.1243 774.7246 690.4657 629.9094 736.2206 ## 2017 1133.3154 853.1596 760.3701 693.6830 810.7573 plot(data,type=&#39;l&#39;,xlim=c(2003,2018),ylim=c(1,1600),xlab = &#39;Year&#39;,ylab = &#39;Tractor Salg&#39;) lines(10^(pred$pred),col=&#39;blue&#39;) lines(10^(pred$pred+2*pred$se),col=&#39;orange&#39;) lines(10^(pred$pred-2*pred$se),col=&#39;orange&#39;) 7.8.2 Detail debet card forbrug på Island (millioner ISK). #Hent fpp pakken og load den plot(debitcards) dldebitcards &lt;- diff(log10(debitcards)) plot(dldebitcards,ylab=&quot;Differenced Log (debitcards)&quot;) require(forecast) ARIMAfit = auto.arima(log10(debitcards), approximation=FALSE,trace=FALSE) ARIMAfit ## Series: log10(debitcards) ## ARIMA(2,1,0)(0,1,1)[12] ## ## Coefficients: ## ar1 ar2 sma1 ## -0.7167 -0.4372 -0.8352 ## s.e. 0.0761 0.0763 0.1085 ## ## sigma^2 estimated as 0.0004402: log likelihood=343.95 ## AIC=-679.9 AICc=-679.61 BIC=-668.05 Nu kan vi forudsige kommende debetkort omsætning med modellen par(mfrow = c(1,1)) pred = predict(ARIMAfit, n.ahead = 36) plot(debitcards,type=&#39;l&#39;,xlim=c(2000,2016),ylim=c(1,40000),xlab = &#39;Year&#39;,ylab = &#39;Debetcard usage&#39;) lines(10^(pred$pred),col=&#39;blue&#39;) lines(10^(pred$pred+2*pred$se),col=&#39;orange&#39;) lines(10^(pred$pred-2*pred$se),col=&#39;orange&#39;) Forudsagt brug af debetkort bliver: 10^(pred$pred) ## Jan Feb Mar Apr May Jun Jul ## 2013 19717.77 19162.87 20436.29 20506.84 23262.14 23545.62 24292.86 ## 2014 20701.39 20352.57 21886.85 21721.53 24745.18 25091.09 25806.49 ## 2015 22017.60 21649.95 23281.85 23104.56 26321.98 26689.74 27450.29 ## Aug Sep Oct Nov Dec ## 2013 25544.16 22267.47 22543.80 22081.63 29090.93 ## 2014 27175.65 23697.15 23970.40 23490.36 30947.83 ## 2015 28907.08 25206.87 25497.43 24986.92 32919.46 7.9 Forecast Aktiekurser .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Man kan hente online aktiekurser med quantmod pakken installer denne med fx. pacman, vi skal også bruge pakken forecast som vi ligeledes henter. Vi henter nedenfor Google justeret lukkekurs til dato det er 6 søjle i GOOG matricen nedenfor. Vi kan se forecaste aktiekursen vha. pacman::p_load(quantmod, forecast) getSymbols(&quot;GOOG&quot;,from = &quot;2017-01-01&quot;, to = Sys.Date(),getSymbols.warning4.0=FALSE) ## [1] &quot;GOOG&quot; plot(GOOG[,6],main = &quot;Google adj. close&quot;) agoog &lt;- auto.arima(GOOG[,6]) agoog ## Series: GOOG[, 6] ## ARIMA(0,1,1) ## ## Coefficients: ## ma1 ## 0.0912 ## s.e. 0.0495 ## ## sigma^2 estimated as 174.5: log likelihood=-1779.46 ## AIC=3562.92 AICc=3562.95 BIC=3571.11 fagoog &lt;- forecast(agoog) fagoog ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 447 1137.958 1121.029 1154.888 1112.067 1163.849 ## 448 1137.958 1112.902 1163.015 1099.638 1176.279 ## 449 1137.958 1106.828 1169.088 1090.349 1185.568 ## 450 1137.958 1101.760 1174.157 1082.598 1193.319 ## 451 1137.958 1097.319 1178.598 1075.806 1200.111 ## 452 1137.958 1093.318 1182.599 1069.686 1206.231 ## 453 1137.958 1089.646 1186.270 1064.072 1211.845 ## 454 1137.958 1086.235 1189.681 1058.855 1217.062 ## 455 1137.958 1083.036 1192.881 1053.961 1221.956 ## 456 1137.958 1080.012 1195.905 1049.337 1226.579 plot(fagoog,main = &quot;Google adj. close&quot;) getSymbols(&quot;GS&quot;,from = &quot;2017-01-01&quot;, to = Sys.Date(),getSymbols.warning4.0=FALSE) ## [1] &quot;GS&quot; plot(GS[,6],main = &quot;Goldman Sachs adj. close&quot;) ags &lt;- auto.arima(GS[,6]) ags ## Series: GS[, 6] ## ARIMA(0,1,0) ## ## sigma^2 estimated as 9.511: log likelihood=-1132.58 ## AIC=2267.17 AICc=2267.18 BIC=2271.26 fags &lt;- forecast(ags) fags ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 447 222.91 218.9578 226.8622 216.8656 228.9544 ## 448 222.91 217.3208 228.4993 214.3620 231.4580 ## 449 222.91 216.0646 229.7554 212.4409 233.3791 ## 450 222.91 215.0056 230.8144 210.8213 234.9987 ## 451 222.91 214.0726 231.7474 209.3944 236.4256 ## 452 222.91 213.2291 232.5909 208.1044 237.7156 ## 453 222.91 212.4535 233.3665 206.9181 238.9019 ## 454 222.91 211.7315 234.0885 205.8140 240.0060 ## 455 222.91 211.0534 234.7666 204.7769 241.0431 ## 456 222.91 210.4121 235.4079 203.7961 242.0239 plot(fags,main = &quot;Goldman Sachs adj. close&quot;) getSymbols(&quot;DANSKE.CO&quot;,from = &quot;2017-01-01&quot;, to = Sys.Date(),getSymbols.warning4.0=FALSE) ## [1] &quot;DANSKE.CO&quot; plot(DANSKE.CO[,6],main = &quot;Danske Bank adj. close&quot;) addb &lt;- auto.arima(DANSKE.CO[,6]) addb ## Series: DANSKE.CO[, 6] ## ARIMA(0,2,1) ## ## Coefficients: ## ma1 ## -0.9882 ## s.e. 0.0067 ## ## sigma^2 estimated as 6.989: log likelihood=-1036.04 ## AIC=2076.09 AICc=2076.12 BIC=2084.26 faddb &lt;- forecast(addb) faddb ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 443 148.1059 144.7179 151.4939 142.9244 153.2874 ## 444 147.5118 142.6922 152.3315 140.1408 154.8829 ## 445 146.9178 140.9802 152.8553 137.8371 155.9985 ## 446 146.3237 139.4274 153.2200 135.7768 156.8706 ## 447 145.7296 137.9744 153.4849 133.8690 157.5902 ## 448 145.1355 136.5907 153.6804 132.0673 158.2037 ## 449 144.5415 135.2586 153.8243 130.3445 158.7384 ## 450 143.9474 133.9663 153.9284 128.6827 159.2120 ## 451 143.3533 132.7060 154.0006 127.0697 159.6369 ## 452 142.7592 131.4718 154.0466 125.4967 160.0218 plot(faddb,main = &quot;Danske Bank adj. close&quot;) getSymbols(&quot;BRK-A&quot;,from = &quot;2000-01-01&quot;, to = Sys.Date(),getSymbols.warning4.0=FALSE) ## [1] &quot;BRK-A&quot; plot(`BRK-A`[,6],main = &quot;Berkshire adj. close&quot;) aberkshire &lt;- auto.arima(`BRK-A`[,6]) aberkshire ## Series: `BRK-A`[, 6] ## ARIMA(1,2,0) ## ## Coefficients: ## ar1 ## -0.5063 ## s.e. 0.0126 ## ## sigma^2 estimated as 4497363: log likelihood=-42858.96 ## AIC=85721.92 AICc=85721.92 BIC=85734.84 faberkshire &lt;- forecast(aberkshire) faberkshire ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 4724 338704.7 335986.9 341422.5 334548.2 342861.2 ## 4725 341383.6 336498.3 346269.0 333912.1 348855.2 ## 4726 344262.9 336449.3 352076.6 332313.0 356212.9 ## 4727 347040.8 336011.7 358069.9 330173.2 363908.4 ## 4728 349870.0 335216.4 364523.6 327459.2 372280.8 ## 4729 352673.2 334093.9 371252.6 324258.6 381087.9 ## 4730 355489.6 332674.3 378304.9 320596.7 390382.5 ## 4731 358299.3 330973.7 385624.9 316508.5 400090.2 ## 4732 361112.4 329011.1 393213.7 312017.7 410207.1 ## 4733 363923.8 326798.7 401048.9 307145.9 420701.7 plot(faberkshire,main = &quot;Berkshire adj. close&quot;) 7.10 Aktieafkast .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } I Quantmod pakken ligger også mulighed for at beregne fx. dagligt, ugentligt afkast, dette gør vi vha. funktionen “periodReturn”. getSymbols(&quot;AAPL&quot;,src=&#39;yahoo&#39;) ## [1] &quot;AAPL&quot; apple &lt;- periodReturn(`AAPL`,period=&#39;yearly&#39;,subset=&#39;2003::&#39;) # Årligt Afkast 2003 til i dag plot(apple, main = &quot;Apple årligt afkast siden 2007&quot;) auto.arima(apple) ## Series: apple ## ARIMA(1,0,0) with non-zero mean ## ## Coefficients: ## ar1 mean ## -0.5973 0.3529 ## s.e. 0.2499 0.0815 ## ## sigma^2 estimated as 0.2264: log likelihood=-7.24 ## AIC=20.48 AICc=23.48 BIC=21.94 getSymbols(&quot;BRK-A&quot;,src=&#39;yahoo&#39;) ## [1] &quot;BRK-A&quot; berkshire &lt;- periodReturn(`BRK-A`,period=&#39;yearly&#39;,subset=&#39;2003::&#39;) plot(berkshire, main = &quot;Berkshire årligt afkast siden 2007&quot;) auto.arima(berkshire) ## Series: berkshire ## ARIMA(0,0,0) with non-zero mean ## ## Coefficients: ## mean ## 0.1150 ## s.e. 0.0536 ## ## sigma^2 estimated as 0.03763: log likelihood=3.17 ## AIC=-2.35 AICc=-1.01 BIC=-1.38 7.10.1 ARIMA opsamling video .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } "]
]
